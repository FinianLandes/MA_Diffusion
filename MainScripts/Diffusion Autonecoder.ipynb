{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Diffusion Script\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    import librosa\n",
    "except:\n",
    "    !pip install librosa\n",
    "try:\n",
    "    from audio_diffusion_pytorch import DiffusionModel, UNetV0, LTPlugin, VDiffusion, VSampler\n",
    "    from audio_encoders_pytorch import MelE1d, TanhBottleneck\n",
    "except:\n",
    "    !pip install audio-diffusion-pytorch\n",
    "    !pip install audio-encoders-pytorch\n",
    "try: \n",
    "    import a_unet\n",
    "except:\n",
    "    !pip install a_unet\n",
    "\n",
    "#Set Dir \n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "from audio_diffusion_pytorch import DiffusionModel, DiffusionAE, VDiffusion, VSampler, UNetV0, LTPlugin\n",
    "from audio_encoders_pytorch import MelE1d, TanhBottleneck, ME1d\n",
    "\n",
    "# Utils\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import logging\n",
    "from typing import Sequence, Optional, Callable\n",
    "\n",
    "\n",
    "# Base Scripts\n",
    "from Libraries.Utils import *\n",
    "from MainScripts.Conf import conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_kernel: bool = True #Set to true if using a remote Kernel changes the file structure\n",
    "model_name: str = \"diffusion_autoencoder_v10\"\n",
    "training_data_name: str = \"training_full_wave\"\n",
    "full_model_path: str = path_to_remote_path(\"{}/{}\".format(conf[\"paths\"].model_path, model_name + \".pth\"), remote_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_level: int = LIGHT_DEBUG\n",
    "logging.basicConfig(level=logging_level, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger: logging.Logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_training_samples: int = 2500\n",
    "n_validation_samples: int = 50\n",
    "batch_size: int = 16\n",
    "tensor_dim: list = [batch_size, 1, 65636 * 2] #B, C, H = Batch, channels, Time domain\n",
    "\n",
    "learning_rate: float = 1e-4\n",
    "epochs: int = 300\n",
    "restart_training: bool = True\n",
    "checkpoint_freq: int = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 06:41:18,944 - LIGHT_DEBUG - Ndarray loaded from Data/training_full_wave.npy of shape: (5906, 147200)\n",
      "2025-06-04 06:41:19,946 - INFO - Data loaded with shape: (5906, 131072)\n"
     ]
    }
   ],
   "source": [
    "file: ndarray = load_training_data(path_to_remote_path(\"{}/{}\".format(conf[\"paths\"].data_path, training_data_name + \".npy\"), remote_kernel))[:, :2**17]\n",
    "np.random.seed(50)\n",
    "np.random.shuffle(file)\n",
    "data_loader = create_dataloader(Audio_Data(file[:n_training_samples]), batch_size)\n",
    "validation_dataloader = create_dataloader(Audio_Data(file[-n_validation_samples:]), batch_size)\n",
    "logger.info(f\"Data loaded with shape: {file.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = DiffusionAE(\n",
    "    encoder=ME1d( \n",
    "        in_channels=1,\n",
    "        use_log=True,\n",
    "        stft_num_fft=1023,\n",
    "        stft_hop_length=8,\n",
    "        channels=128,\n",
    "        multipliers=[1, 1],\n",
    "        factors=[2],\n",
    "        num_blocks=[12],\n",
    "        bottleneck=TanhBottleneck(),\n",
    "        out_channels=256,\n",
    "    ),\n",
    "    inject_depth=4,\n",
    "    net_t=UNetV0,\n",
    "    in_channels=1,\n",
    "    channels=[32, 64, 64, 128, 128, 256, 256], \n",
    "    factors=[1, 2, 2, 2, 2, 2, 2],\n",
    "    items=[1, 2, 2, 2, 2, 2, 2],\n",
    "    diffusion_t=VDiffusion,\n",
    "    sampler_t=VSampler, \n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 06:41:25,450 - INFO - Model diffusion_autoencoder_v10 loaded with ~16.10M Parameters\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(autoencoder.parameters(), lr=learning_rate, weight_decay=1e-3, betas=[0.95, 0.999], eps=1e-6)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "start_epoch: int = 0\n",
    "if os.path.exists(full_model_path):\n",
    "    model = torch.load(full_model_path, map_location=device)\n",
    "    autoencoder.load_state_dict(model[\"model\"])\n",
    "    if not restart_training:\n",
    "        optimizer.load_state_dict(model[\"optim\"])\n",
    "        scheduler.load_state_dict(model[\"scheduler\"])\n",
    "        start_epoch = model.get(\"epoch\", 0)\n",
    "    logger.info(f\"Model {model_name} loaded with {count_parameters(autoencoder)} Parameters\")\n",
    "else: \n",
    "    logger.info(f\"Model {model_name} created with {count_parameters(autoencoder)} Parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(autoencoder, optimizer, scheduler, device, n_dims=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:40:41,825 - INFO - Training started on cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:43:01,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.106 Min/Max params: -2.424, 2.806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:43:03,050 - INFO - Epoch 001: Avg. Loss: 1.97604e-01 Avg. val. Loss: 1.30283e-01 Remaining Time: 11h 43min 44s LR: 9.80000e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:45:26,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.394 Min/Max params: -2.423, 2.805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:45:27,878 - INFO - Epoch 002: Avg. Loss: 1.23895e-01 Avg. val. Loss: 9.58574e-02 Remaining Time: 11h 50min 20s LR: 9.60400e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:47:48,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.064 Min/Max params: -2.422, 2.805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:47:50,246 - INFO - Epoch 003: Avg. Loss: 1.19410e-01 Avg. val. Loss: 8.77099e-02 Remaining Time: 11h 46min 52s LR: 9.41192e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:50:13,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.056 Min/Max params: -2.421, 2.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:50:15,007 - INFO - Epoch 004: Avg. Loss: 1.25156e-01 Avg. val. Loss: 9.61044e-02 Remaining Time: 11h 46min 54s LR: 9.22368e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:52:35,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.112 Min/Max params: -2.421, 2.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:52:37,013 - INFO - Epoch 005: Avg. Loss: 1.25080e-01 Avg. val. Loss: 1.11967e-01 Remaining Time: 11h 43min 15s LR: 9.03921e-05 \n",
      "2025-06-03 18:52:37,287 - LIGHT_DEBUG - Checkpoint saved model to Models/diffusion_autoencoder_v10_epoch_005.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:54:58,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.065 Min/Max params: -2.422, 2.804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:54:59,343 - INFO - Epoch 006: Avg. Loss: 1.24935e-01 Avg. val. Loss: 2.15538e-01 Remaining Time: 11h 40min 04s LR: 8.85842e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:57:23,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.073 Min/Max params: -2.424, 2.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:57:24,646 - INFO - Epoch 007: Avg. Loss: 1.26312e-01 Avg. val. Loss: 1.26821e-01 Remaining Time: 11h 39min 23s LR: 8.68126e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:59:43,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.103 Min/Max params: -2.427, 2.810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:59:45,199 - INFO - Epoch 008: Avg. Loss: 1.34014e-01 Avg. val. Loss: 1.21257e-01 Remaining Time: 11h 35min 22s LR: 8.50763e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:02:08,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.244 Min/Max params: -2.424, 2.808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:02:09,384 - INFO - Epoch 009: Avg. Loss: 1.49571e-01 Avg. val. Loss: 1.18277e-01 Remaining Time: 11h 33min 41s LR: 8.33748e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:04:31,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.141 Min/Max params: -2.424, 2.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:04:33,054 - INFO - Epoch 010: Avg. Loss: 1.39664e-01 Avg. val. Loss: 1.50048e-01 Remaining Time: 11h 31min 37s LR: 8.17073e-05 \n",
      "2025-06-03 19:04:33,348 - LIGHT_DEBUG - Models/diffusion_autoencoder_v10_epoch_005.pth deleted\n",
      "2025-06-03 19:04:33,349 - LIGHT_DEBUG - Checkpoint saved model to Models/diffusion_autoencoder_v10_epoch_010.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:06:55,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.084 Min/Max params: -2.424, 2.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:06:56,926 - INFO - Epoch 011: Avg. Loss: 1.34078e-01 Avg. val. Loss: 1.21003e-01 Remaining Time: 11h 29min 26s LR: 8.00731e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:09:19,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.094 Min/Max params: -2.425, 2.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:09:20,213 - INFO - Epoch 012: Avg. Loss: 1.34470e-01 Avg. val. Loss: 1.38854e-01 Remaining Time: 11h 27min 06s LR: 7.84717e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:11:43,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.133 Min/Max params: -2.424, 2.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:11:44,667 - INFO - Epoch 013: Avg. Loss: 1.33166e-01 Avg. val. Loss: 1.33064e-01 Remaining Time: 11h 25min 12s LR: 7.69022e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:14:08,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.119 Min/Max params: -2.426, 2.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:14:09,840 - INFO - Epoch 014: Avg. Loss: 1.35123e-01 Avg. val. Loss: 1.68136e-01 Remaining Time: 11h 23min 28s LR: 7.53642e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:16:29,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.142 Min/Max params: -2.427, 2.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:16:30,849 - INFO - Epoch 015: Avg. Loss: 1.39290e-01 Avg. val. Loss: 1.11689e-01 Remaining Time: 11h 20min 19s LR: 7.38569e-05 \n",
      "2025-06-03 19:16:31,276 - LIGHT_DEBUG - Models/diffusion_autoencoder_v10_epoch_010.pth deleted\n",
      "2025-06-03 19:16:31,277 - LIGHT_DEBUG - Checkpoint saved model to Models/diffusion_autoencoder_v10_epoch_015.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:18:53,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.095 Min/Max params: -2.428, 2.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:18:55,138 - INFO - Epoch 016: Avg. Loss: 1.28560e-01 Avg. val. Loss: 1.05822e-01 Remaining Time: 11h 18min 07s LR: 7.23798e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:21:14,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.128 Min/Max params: -2.427, 2.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:21:15,809 - INFO - Epoch 017: Avg. Loss: 1.47642e-01 Avg. val. Loss: 1.45102e-01 Remaining Time: 11h 15min 01s LR: 7.09322e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:23:37,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.243 Min/Max params: -2.427, 2.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:23:39,148 - INFO - Epoch 018: Avg. Loss: 1.30285e-01 Avg. val. Loss: 1.67507e-01 Remaining Time: 11h 12min 41s LR: 6.95135e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:26:04,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.098 Min/Max params: -2.427, 2.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:26:06,723 - INFO - Epoch 019: Avg. Loss: 1.31148e-01 Avg. val. Loss: 1.28291e-01 Remaining Time: 11h 11min 24s LR: 6.81233e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:28:31,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.208 Min/Max params: -2.427, 2.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:28:33,264 - INFO - Epoch 020: Avg. Loss: 1.31773e-01 Avg. val. Loss: 1.38520e-01 Remaining Time: 11h 09min 45s LR: 6.67608e-05 \n",
      "2025-06-03 19:28:33,746 - LIGHT_DEBUG - Models/diffusion_autoencoder_v10_epoch_015.pth deleted\n",
      "2025-06-03 19:28:33,747 - LIGHT_DEBUG - Checkpoint saved model to Models/diffusion_autoencoder_v10_epoch_020.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:30:54,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.387 Min/Max params: -2.429, 2.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:30:56,004 - INFO - Epoch 021: Avg. Loss: 1.78958e-01 Avg. val. Loss: 1.88850e-01 Remaining Time: 11h 07min 05s LR: 6.54256e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:33:19,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.073 Min/Max params: -2.428, 2.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:33:21,501 - INFO - Epoch 022: Avg. Loss: 1.41661e-01 Avg. val. Loss: 1.22971e-01 Remaining Time: 11h 05min 07s LR: 6.41171e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:35:45,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.054 Min/Max params: -2.427, 2.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:35:47,553 - INFO - Epoch 023: Avg. Loss: 1.31480e-01 Avg. val. Loss: 1.32128e-01 Remaining Time: 11h 03min 13s LR: 6.28347e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:38:11,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.083 Min/Max params: -2.426, 2.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:38:12,432 - INFO - Epoch 024: Avg. Loss: 1.43414e-01 Avg. val. Loss: 1.06254e-01 Remaining Time: 11h 01min 04s LR: 6.15780e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:40:32,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.074 Min/Max params: -2.424, 2.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:40:33,326 - INFO - Epoch 025: Avg. Loss: 1.28756e-01 Avg. val. Loss: 2.21887e-01 Remaining Time: 10h 58min 09s LR: 6.03465e-05 \n",
      "2025-06-03 19:40:33,617 - LIGHT_DEBUG - Models/diffusion_autoencoder_v10_epoch_020.pth deleted\n",
      "2025-06-03 19:40:33,618 - LIGHT_DEBUG - Checkpoint saved model to Models/diffusion_autoencoder_v10_epoch_025.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:42:54,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.310 Min/Max params: -2.430, 2.8128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:42:55,314 - INFO - Epoch 026: Avg. Loss: 6.34824e-01 Avg. val. Loss: 4.02205e-01 Remaining Time: 10h 55min 25s LR: 5.91395e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:45:16,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.303 Min/Max params: -2.430, 2.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:45:17,823 - INFO - Epoch 027: Avg. Loss: 2.22196e-01 Avg. val. Loss: 1.43489e-01 Remaining Time: 10h 52min 52s LR: 5.79568e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:47:39,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.068 Min/Max params: -2.430, 2.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:47:40,908 - INFO - Epoch 028: Avg. Loss: 1.37127e-01 Avg. val. Loss: 1.32188e-01 Remaining Time: 10h 50min 24s LR: 5.67976e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:50:03,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.069 Min/Max params: -2.430, 2.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:50:05,502 - INFO - Epoch 029: Avg. Loss: 1.27747e-01 Avg. val. Loss: 1.18804e-01 Remaining Time: 10h 48min 11s LR: 5.56617e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:52:24,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.174 Min/Max params: -2.430, 2.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:52:25,527 - INFO - Epoch 030: Avg. Loss: 1.29955e-01 Avg. val. Loss: 1.39447e-01 Remaining Time: 10h 45min 16s LR: 5.45484e-05 \n",
      "2025-06-03 19:52:25,816 - LIGHT_DEBUG - Models/diffusion_autoencoder_v10_epoch_025.pth deleted\n",
      "2025-06-03 19:52:25,817 - LIGHT_DEBUG - Checkpoint saved model to Models/diffusion_autoencoder_v10_epoch_030.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:54:49,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.136 Min/Max params: -2.430, 2.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:54:50,641 - INFO - Epoch 031: Avg. Loss: 1.28316e-01 Avg. val. Loss: 8.81333e-02 Remaining Time: 10h 43min 05s LR: 5.34575e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:57:13,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.122 Min/Max params: -2.430, 2.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:57:15,062 - INFO - Epoch 032: Avg. Loss: 1.25574e-01 Avg. val. Loss: 1.15289e-01 Remaining Time: 10h 40min 50s LR: 5.23883e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:59:37,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.081 Min/Max params: -2.430, 2.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:59:38,553 - INFO - Epoch 033: Avg. Loss: 1.24045e-01 Avg. val. Loss: 1.20144e-01 Remaining Time: 10h 38min 27s LR: 5.13405e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:02:01,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.221 Min/Max params: -2.430, 2.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:02:03,034 - INFO - Epoch 034: Avg. Loss: 1.29677e-01 Avg. val. Loss: 1.11094e-01 Remaining Time: 10h 36min 11s LR: 5.03137e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:04:28,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.116 Min/Max params: -2.430, 2.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:04:29,510 - INFO - Epoch 035: Avg. Loss: 1.27974e-01 Avg. val. Loss: 1.10206e-01 Remaining Time: 10h 34min 10s LR: 4.93075e-05 \n",
      "2025-06-03 20:04:29,944 - LIGHT_DEBUG - Models/diffusion_autoencoder_v10_epoch_030.pth deleted\n",
      "2025-06-03 20:04:29,945 - LIGHT_DEBUG - Checkpoint saved model to Models/diffusion_autoencoder_v10_epoch_035.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:06:52,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.072 Min/Max params: -2.430, 2.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:06:53,932 - INFO - Epoch 036: Avg. Loss: 1.28967e-01 Avg. val. Loss: 1.31783e-01 Remaining Time: 10h 31min 49s LR: 4.83213e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:09:14,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.110 Min/Max params: -2.430, 2.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:09:15,284 - INFO - Epoch 037: Avg. Loss: 1.25813e-01 Avg. val. Loss: 1.19235e-01 Remaining Time: 10h 29min 10s LR: 4.73549e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:11:37,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.324 Min/Max params: -2.429, 2.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:11:38,578 - INFO - Epoch 038: Avg. Loss: 1.28689e-01 Avg. val. Loss: 1.22273e-01 Remaining Time: 10h 26min 45s LR: 4.64078e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:13:58,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.060 Min/Max params: -2.429, 2.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:13:59,755 - INFO - Epoch 039: Avg. Loss: 1.27965e-01 Avg. val. Loss: 1.30583e-01 Remaining Time: 10h 24min 05s LR: 4.54796e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:16:21,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.118 Min/Max params: -2.429, 2.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:16:22,644 - INFO - Epoch 040: Avg. Loss: 1.25664e-01 Avg. val. Loss: 1.44463e-01 Remaining Time: 10h 21min 38s LR: 4.45700e-05 \n",
      "2025-06-03 20:16:22,930 - LIGHT_DEBUG - Models/diffusion_autoencoder_v10_epoch_035.pth deleted\n",
      "2025-06-03 20:16:22,931 - LIGHT_DEBUG - Checkpoint saved model to Models/diffusion_autoencoder_v10_epoch_040.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:18:42,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.084 Min/Max params: -2.429, 2.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:18:43,612 - INFO - Epoch 041: Avg. Loss: 1.31030e-01 Avg. val. Loss: 2.60447e-01 Remaining Time: 10h 18min 57s LR: 4.36786e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:21:05,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.094 Min/Max params: -2.428, 2.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:21:06,259 - INFO - Epoch 042: Avg. Loss: 1.34669e-01 Avg. val. Loss: 1.35630e-01 Remaining Time: 10h 16min 29s LR: 4.28051e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:23:29,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.088 Min/Max params: -2.428, 2.810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:23:30,355 - INFO - Epoch 043: Avg. Loss: 1.37803e-01 Avg. val. Loss: 1.16663e-01 Remaining Time: 10h 14min 10s LR: 4.19490e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:25:55,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.111 Min/Max params: -2.428, 2.810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:25:56,764 - INFO - Epoch 044: Avg. Loss: 1.55098e-01 Avg. val. Loss: 1.23759e-01 Remaining Time: 10h 12min 04s LR: 4.11100e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:28:19,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.291 Min/Max params: -2.428, 2.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:28:21,358 - INFO - Epoch 045: Avg. Loss: 1.69939e-01 Avg. val. Loss: 2.00617e-01 Remaining Time: 10h 09min 47s LR: 4.02878e-05 \n",
      "2025-06-03 20:28:21,637 - LIGHT_DEBUG - Models/diffusion_autoencoder_v10_epoch_040.pth deleted\n",
      "2025-06-03 20:28:21,638 - LIGHT_DEBUG - Checkpoint saved model to Models/diffusion_autoencoder_v10_epoch_045.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:30:43,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.985 Min/Max params: -2.430, 2.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:30:45,145 - INFO - Epoch 046: Avg. Loss: 4.53458e-01 Avg. val. Loss: 2.05903e-01 Remaining Time: 10h 07min 24s LR: 3.94820e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:33:08,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.067 Min/Max params: -2.431, 2.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:33:09,843 - INFO - Epoch 047: Avg. Loss: 2.66240e-01 Avg. val. Loss: 3.12246e-01 Remaining Time: 10h 05min 07s LR: 3.86924e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:35:31,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.232 Min/Max params: -2.431, 2.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:35:32,427 - INFO - Epoch 048: Avg. Loss: 2.28847e-01 Avg. val. Loss: 2.25289e-01 Remaining Time: 10h 02min 39s LR: 3.79185e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:37:56,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.236 Min/Max params: -2.432, 2.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:37:57,732 - INFO - Epoch 049: Avg. Loss: 1.85301e-01 Avg. val. Loss: 1.61603e-01 Remaining Time: 10h 00min 24s LR: 3.71602e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:40:19,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.291 Min/Max params: -2.433, 2.815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:40:21,092 - INFO - Epoch 050: Avg. Loss: 1.62392e-01 Avg. val. Loss: 1.82482e-01 Remaining Time: 09h 58min 00s LR: 3.64170e-05 \n",
      "2025-06-03 20:40:21,358 - LIGHT_DEBUG - Models/diffusion_autoencoder_v10_epoch_045.pth deleted\n",
      "2025-06-03 20:40:21,359 - LIGHT_DEBUG - Checkpoint saved model to Models/diffusion_autoencoder_v10_epoch_050.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:42:42,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.151 Min/Max params: -2.434, 2.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:42:43,853 - INFO - Epoch 051: Avg. Loss: 1.63304e-01 Avg. val. Loss: 1.40279e-01 Remaining Time: 09h 55min 31s LR: 3.56886e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:45:03,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.086 Min/Max params: -2.434, 2.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:45:05,142 - INFO - Epoch 052: Avg. Loss: 1.52249e-01 Avg. val. Loss: 2.21618e-01 Remaining Time: 09h 52min 57s LR: 3.49749e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:47:27,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.112 Min/Max params: -2.435, 2.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:47:28,590 - INFO - Epoch 053: Avg. Loss: 1.57060e-01 Avg. val. Loss: 2.05116e-01 Remaining Time: 09h 50min 34s LR: 3.42754e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:49:50,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.236 Min/Max params: -2.435, 2.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:49:51,522 - INFO - Epoch 054: Avg. Loss: 1.56267e-01 Avg. val. Loss: 2.02963e-01 Remaining Time: 09h 48min 08s LR: 3.35899e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:52:13,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.053 Min/Max params: -2.435, 2.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:52:15,418 - INFO - Epoch 055: Avg. Loss: 1.59550e-01 Avg. val. Loss: 1.26099e-01 Remaining Time: 09h 45min 46s LR: 3.29181e-05 \n",
      "2025-06-03 20:52:15,754 - LIGHT_DEBUG - Models/diffusion_autoencoder_v10_epoch_050.pth deleted\n",
      "2025-06-03 20:52:15,755 - LIGHT_DEBUG - Checkpoint saved model to Models/diffusion_autoencoder_v10_epoch_055.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:54:42,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.082 Min/Max params: -2.435, 2.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:54:44,113 - INFO - Epoch 056: Avg. Loss: 1.52511e-01 Avg. val. Loss: 1.25287e-01 Remaining Time: 09h 43min 44s LR: 3.22597e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:57:05,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.174 Min/Max params: -2.435, 2.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:57:07,738 - INFO - Epoch 057: Avg. Loss: 1.42514e-01 Avg. val. Loss: 1.35719e-01 Remaining Time: 09h 41min 21s LR: 3.16145e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:59:28,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.067 Min/Max params: -2.436, 2.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:59:30,546 - INFO - Epoch 058: Avg. Loss: 1.50395e-01 Avg. val. Loss: 1.92180e-01 Remaining Time: 09h 38min 55s LR: 3.09822e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:01:51,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.120 Min/Max params: -2.436, 2.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:01:52,452 - INFO - Epoch 059: Avg. Loss: 1.54133e-01 Avg. val. Loss: 1.70367e-01 Remaining Time: 09h 36min 24s LR: 3.03626e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:04:14,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.283 Min/Max params: -2.436, 2.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:04:15,444 - INFO - Epoch 060: Avg. Loss: 1.53388e-01 Avg. val. Loss: 1.81147e-01 Remaining Time: 09h 33min 59s LR: 2.97553e-05 \n",
      "2025-06-03 21:04:15,709 - LIGHT_DEBUG - Models/diffusion_autoencoder_v10_epoch_055.pth deleted\n",
      "2025-06-03 21:04:15,710 - LIGHT_DEBUG - Checkpoint saved model to Models/diffusion_autoencoder_v10_epoch_060.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:06:36,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.111 Min/Max params: -2.436, 2.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:06:37,945 - INFO - Epoch 061: Avg. Loss: 1.51503e-01 Avg. val. Loss: 1.80611e-01 Remaining Time: 09h 31min 30s LR: 2.91602e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:08:59,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.087 Min/Max params: -2.436, 2.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:09:00,761 - INFO - Epoch 062: Avg. Loss: 1.50435e-01 Avg. val. Loss: 1.14303e-01 Remaining Time: 09h 29min 04s LR: 2.85770e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:11:20,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.130 Min/Max params: -2.437, 2.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:11:22,365 - INFO - Epoch 063: Avg. Loss: 1.42478e-01 Avg. val. Loss: 1.55639e-01 Remaining Time: 09h 26min 34s LR: 2.80055e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:13:44,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.077 Min/Max params: -2.437, 2.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:13:45,995 - INFO - Epoch 064: Avg. Loss: 1.49104e-01 Avg. val. Loss: 1.04673e-01 Remaining Time: 09h 24min 11s LR: 2.74454e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:16:08,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.083 Min/Max params: -2.437, 2.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:16:10,057 - INFO - Epoch 065: Avg. Loss: 1.45340e-01 Avg. val. Loss: 1.51606e-01 Remaining Time: 09h 21min 50s LR: 2.68964e-05 \n",
      "2025-06-03 21:16:10,335 - LIGHT_DEBUG - Models/diffusion_autoencoder_v10_epoch_060.pth deleted\n",
      "2025-06-03 21:16:10,336 - LIGHT_DEBUG - Checkpoint saved model to Models/diffusion_autoencoder_v10_epoch_065.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:18:32,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.100 Min/Max params: -2.437, 2.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:18:33,735 - INFO - Epoch 066: Avg. Loss: 1.52175e-01 Avg. val. Loss: 1.26450e-01 Remaining Time: 09h 19min 26s LR: 2.63585e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:20:57,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.070 Min/Max params: -2.436, 2.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:20:58,794 - INFO - Epoch 067: Avg. Loss: 1.38002e-01 Avg. val. Loss: 1.44147e-01 Remaining Time: 09h 17min 08s LR: 2.58313e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:23:21,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.261 Min/Max params: -2.436, 2.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:23:23,557 - INFO - Epoch 068: Avg. Loss: 1.44323e-01 Avg. val. Loss: 1.67343e-01 Remaining Time: 09h 14min 49s LR: 2.53147e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:25:45,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.105 Min/Max params: -2.437, 2.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:25:46,673 - INFO - Epoch 069: Avg. Loss: 1.42623e-01 Avg. val. Loss: 1.20154e-01 Remaining Time: 09h 12min 25s LR: 2.48084e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:28:12,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.134 Min/Max params: -2.437, 2.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:28:13,490 - INFO - Epoch 070: Avg. Loss: 1.43565e-01 Avg. val. Loss: 2.36796e-01 Remaining Time: 09h 10min 12s LR: 2.43123e-05 \n",
      "2025-06-03 21:28:13,788 - LIGHT_DEBUG - Models/diffusion_autoencoder_v10_epoch_065.pth deleted\n",
      "2025-06-03 21:28:13,789 - LIGHT_DEBUG - Checkpoint saved model to Models/diffusion_autoencoder_v10_epoch_070.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:30:33,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.059 Min/Max params: -2.437, 2.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:30:34,922 - INFO - Epoch 071: Avg. Loss: 1.50171e-01 Avg. val. Loss: 1.06308e-01 Remaining Time: 09h 07min 41s LR: 2.38260e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:32:58,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.114 Min/Max params: -2.437, 2.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:33:00,415 - INFO - Epoch 072: Avg. Loss: 1.44480e-01 Avg. val. Loss: 1.12880e-01 Remaining Time: 09h 05min 24s LR: 2.33495e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:35:25,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.155 Min/Max params: -2.437, 2.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:35:27,054 - INFO - Epoch 073: Avg. Loss: 1.40290e-01 Avg. val. Loss: 1.32170e-01 Remaining Time: 09h 03min 10s LR: 2.28825e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:37:45,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.058 Min/Max params: -2.437, 2.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:37:48,086 - INFO - Epoch 074: Avg. Loss: 1.39961e-01 Avg. val. Loss: 1.30441e-01 Remaining Time: 09h 00min 38s LR: 2.24249e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:40:07,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.058 Min/Max params: -2.438, 2.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:40:08,435 - INFO - Epoch 075: Avg. Loss: 1.40312e-01 Avg. val. Loss: 1.19446e-01 Remaining Time: 08h 58min 05s LR: 2.19764e-05 \n",
      "2025-06-03 21:40:08,846 - LIGHT_DEBUG - Models/diffusion_autoencoder_v10_epoch_070.pth deleted\n",
      "2025-06-03 21:40:08,847 - LIGHT_DEBUG - Checkpoint saved model to Models/diffusion_autoencoder_v10_epoch_075.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:42:29,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.114 Min/Max params: -2.438, 2.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:42:30,894 - INFO - Epoch 076: Avg. Loss: 1.40268e-01 Avg. val. Loss: 2.44517e-01 Remaining Time: 08h 55min 38s LR: 2.15368e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:44:52,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.098 Min/Max params: -2.438, 2.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:44:53,313 - INFO - Epoch 077: Avg. Loss: 1.45744e-01 Avg. val. Loss: 1.13869e-01 Remaining Time: 08h 53min 11s LR: 2.11061e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:47:15,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.132 Min/Max params: -2.437, 2.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:47:17,593 - INFO - Epoch 078: Avg. Loss: 1.42313e-01 Avg. val. Loss: 1.30390e-01 Remaining Time: 08h 50min 50s LR: 2.06840e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:49:41,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.070 Min/Max params: -2.437, 2.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:49:43,159 - INFO - Epoch 079: Avg. Loss: 1.39499e-01 Avg. val. Loss: 1.34481e-01 Remaining Time: 08h 48min 32s LR: 2.02703e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:52:04,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.074 Min/Max params: -2.436, 2.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:52:05,555 - INFO - Epoch 080: Avg. Loss: 1.37832e-01 Avg. val. Loss: 1.04637e-01 Remaining Time: 08h 46min 06s LR: 1.98649e-05 \n",
      "2025-06-03 21:52:05,827 - LIGHT_DEBUG - Models/diffusion_autoencoder_v10_epoch_075.pth deleted\n",
      "2025-06-03 21:52:05,828 - LIGHT_DEBUG - Checkpoint saved model to Models/diffusion_autoencoder_v10_epoch_080.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:54:27,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.077 Min/Max params: -2.436, 2.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:54:28,737 - INFO - Epoch 081: Avg. Loss: 1.40133e-01 Avg. val. Loss: 1.61154e-01 Remaining Time: 08h 43min 41s LR: 1.94676e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:56:51,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.118 Min/Max params: -2.436, 2.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:56:53,317 - INFO - Epoch 082: Avg. Loss: 1.39598e-01 Avg. val. Loss: 1.43658e-01 Remaining Time: 08h 41min 20s LR: 1.90782e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:59:16,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.255 Min/Max params: -2.436, 2.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:59:18,013 - INFO - Epoch 083: Avg. Loss: 1.44549e-01 Avg. val. Loss: 1.41826e-01 Remaining Time: 08h 39min 00s LR: 1.86967e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 22:01:38,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.118 Min/Max params: -2.436, 2.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 22:01:39,381 - INFO - Epoch 084: Avg. Loss: 1.43226e-01 Avg. val. Loss: 1.32336e-01 Remaining Time: 08h 36min 31s LR: 1.83227e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 22:04:02,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.080 Min/Max params: -2.436, 2.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 22:04:04,431 - INFO - Epoch 085: Avg. Loss: 1.50501e-01 Avg. val. Loss: 1.27363e-01 Remaining Time: 08h 34min 11s LR: 1.79563e-05 \n",
      "2025-06-03 22:04:04,696 - LIGHT_DEBUG - Models/diffusion_autoencoder_v10_epoch_080.pth deleted\n",
      "2025-06-03 22:04:04,696 - LIGHT_DEBUG - Checkpoint saved model to Models/diffusion_autoencoder_v10_epoch_085.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 22:06:27,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.212 Min/Max params: -2.436, 2.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 22:06:28,823 - INFO - Epoch 086: Avg. Loss: 2.83881e-01 Avg. val. Loss: 3.32253e-01 Remaining Time: 08h 31min 49s LR: 1.75972e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 22:08:51,000 - LIGHT_DEBUG - Batch 157/157 Loss: 0.459 Min/Max params: -2.436, 2.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 22:08:52,543 - INFO - Epoch 087: Avg. Loss: 3.02306e-01 Avg. val. Loss: 2.25740e-01 Remaining Time: 08h 29min 26s LR: 1.72452e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 22:09:28,000 - LIGHT_DEBUG - Batch 040/157 Loss: 0.293 Min/Max params: -2.436, 2.817"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 22:09:29,804 - INFO - Breaking due to NaN loss.\n",
      "2025-06-03 22:09:30,101 - LIGHT_DEBUG - Saved model to Models/diffusion_autoencoder_v10.pth\n",
      "2025-06-03 22:09:30,103 - LIGHT_DEBUG - Models/diffusion_autoencoder_v10_epoch_085.pth deleted\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss, val_loss = trainer.train(train_dataset=data_loader,\n",
    "            val_dataset=validation_dataloader,\n",
    "            n_epochs=epochs,\n",
    "            full_model_path=full_model_path,\n",
    "            checkpoint_freq=checkpoint_freq,\n",
    "            patience=-1,\n",
    "            gradient_clip_norm=None,\n",
    "            gradient_clip_val=None,\n",
    "            sample_freq=None,\n",
    "            #use_embed=False\n",
    "            )\n",
    "scatter_plot(train_loss)\n",
    "scatter_plot(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/einops/packing.py:148: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  lengths_of_composed_axes: List[int] = [-1 if -1 in p_shape else prod(p_shape) for p_shape in packed_shapes]\n",
      "/usr/local/lib/python3.11/dist-packages/einops/packing.py:150: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  n_unknown_composed_axes = sum(int(x == -1) for x in lengths_of_composed_axes)\n",
      "/usr/local/lib/python3.11/dist-packages/a_unet/apex.py:276: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert torch.is_tensor(context) and context.shape == shape, msg\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Required context `channels` at depth 4 in forward to be tensor of shape torch.Size([16, 256, 8204]), found torch.Size([16, 256, 8205])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_architecture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_to_remote_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpaths\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremote_kernel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/notebooks/Libraries/Utils.py:702\u001b[0m, in \u001b[0;36mTrainer.save_architecture\u001b[0;34m(self, tensor_dim, path)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    701\u001b[0m     example_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m*\u001b[39mtensor_dim)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 702\u001b[0m     script_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexample_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39msave(script_model, path)\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/jit/_trace.py:798\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_kwarg_inputs should be a dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 798\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs_is_kwarg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m ):\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m example_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/jit/_trace.py:1065\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1064\u001b[0m     example_inputs \u001b[38;5;241m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m-> 1065\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_method_from_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar_lookup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43margument_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1076\u001b[0m check_trace_method \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_c\u001b[38;5;241m.\u001b[39m_get_method(method_name)\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;66;03m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/audio_diffusion_pytorch/models.py:106\u001b[0m, in \u001b[0;36mDiffusionAE.forward\u001b[0;34m(self, x, with_info, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter\u001b[38;5;241m.\u001b[39mencode(x) \u001b[38;5;28;01mif\u001b[39;00m exists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter) \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Compute diffusion loss\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (loss, info) \u001b[38;5;28;01mif\u001b[39;00m with_info \u001b[38;5;28;01melse\u001b[39;00m loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/audio_diffusion_pytorch/models.py:40\u001b[0m, in \u001b[0;36mDiffusionModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffusion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/audio_diffusion_pytorch/diffusion.py:93\u001b[0m, in \u001b[0;36mVDiffusion.forward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m v_target \u001b[38;5;241m=\u001b[39m alphas \u001b[38;5;241m*\u001b[39m noise \u001b[38;5;241m-\u001b[39m betas \u001b[38;5;241m*\u001b[39m x\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Predict velocity and return loss\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m v_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmse_loss(v_pred, v_target)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/a_unet/blocks.py:63\u001b[0m, in \u001b[0;36mModule.<locals>.Module.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/a_unet/blocks.py:594\u001b[0m, in \u001b[0;36mTimeConditioningPlugin.<locals>.Net.<locals>.forward\u001b[0;34m(x, time, features, **kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# Merge time features with features if provided\u001b[39;00m\n\u001b[1;32m    593\u001b[0m features \u001b[38;5;241m=\u001b[39m features \u001b[38;5;241m+\u001b[39m time_features \u001b[38;5;28;01mif\u001b[39;00m exists(features) \u001b[38;5;28;01melse\u001b[39;00m time_features\n\u001b[0;32m--> 594\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/a_unet/apex.py:431\u001b[0m, in \u001b[0;36mXUNet.forward\u001b[0;34m(self, x, features, embedding, channels)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    425\u001b[0m     x: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m     channels: Optional[Sequence[Tensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/a_unet/apex.py:382\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, features, embedding, channels)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    376\u001b[0m     x: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m     channels: Optional[Sequence[Tensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    380\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    381\u001b[0m     skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_adapter(x)\n\u001b[0;32m--> 382\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip(skip, x, features)\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/a_unet/blocks.py:77\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m---> 77\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "    \u001b[0;31m[... skipping similar frames: Module._call_impl at line 1527 (1 times), Module._slow_forward at line 1508 (1 times), Module._wrapped_call_impl at line 1518 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/a_unet/apex.py:382\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, features, embedding, channels)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    376\u001b[0m     x: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m     channels: Optional[Sequence[Tensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    380\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    381\u001b[0m     skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_adapter(x)\n\u001b[0;32m--> 382\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip(skip, x, features)\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "    \u001b[0;31m[... skipping similar frames: Module._call_impl at line 1527 (1 times), Module._slow_forward at line 1508 (1 times), Module._wrapped_call_impl at line 1518 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/a_unet/blocks.py:77\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m---> 77\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "    \u001b[0;31m[... skipping similar frames: Module._call_impl at line 1527 (5 times), Module._slow_forward at line 1508 (5 times), Module._wrapped_call_impl at line 1518 (5 times), Block.forward at line 382 (2 times), Sequential.forward at line 77 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/a_unet/apex.py:382\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, features, embedding, channels)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    376\u001b[0m     x: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m     channels: Optional[Sequence[Tensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    380\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    381\u001b[0m     skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_adapter(x)\n\u001b[0;32m--> 382\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip(skip, x, features)\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "    \u001b[0;31m[... skipping similar frames: Module._call_impl at line 1527 (1 times), Module._slow_forward at line 1508 (1 times), Module._wrapped_call_impl at line 1518 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/a_unet/blocks.py:77\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m---> 77\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/a_unet/blocks.py:92\u001b[0m, in \u001b[0;36mSelect.<locals>.fn.<locals>.Select.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/a_unet/blocks.py:63\u001b[0m, in \u001b[0;36mModule.<locals>.Module.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/a_unet/apex.py:276\u001b[0m, in \u001b[0;36mInjectChannelsItem.<locals>.forward\u001b[0;34m(x, channels)\u001b[0m\n\u001b[1;32m    274\u001b[0m shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize([x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], context_channels, \u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]])\n\u001b[1;32m    275\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequired \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to be tensor of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 276\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(context) \u001b[38;5;129;01mand\u001b[39;00m context\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m shape, msg\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conv(torch\u001b[38;5;241m.\u001b[39mcat([x, context], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m+\u001b[39m x\n",
      "\u001b[0;31mAssertionError\u001b[0m: Required context `channels` at depth 4 in forward to be tensor of shape torch.Size([16, 256, 8204]), found torch.Size([16, 256, 8205])"
     ]
    }
   ],
   "source": [
    "trainer.save_architecture(tensor_dim, path_to_remote_path(\"{}/{}\".format(conf[\"paths\"].result_path, model_name + \".pt\"), remote_kernel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 131072)\n"
     ]
    }
   ],
   "source": [
    "samples = trainer.sample_AE(file[10], n_steps=100)\n",
    "print(samples[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 06:41:43,593 - LIGHT_DEBUG - Normalized to range: [-0.99999,0.99999]\n",
      "2025-06-04 06:41:43,607 - LIGHT_DEBUG - Saved file to:Results/model_result1.wav\n"
     ]
    }
   ],
   "source": [
    "save_audio_file(samples[0, 0], path_to_remote_path(\"{}/{}\".format(conf[\"paths\"].result_path, \"model_result1.wav\"), remote_kernel), 32000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

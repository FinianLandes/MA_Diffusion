{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0896dd2a",
   "metadata": {},
   "source": [
    "# Main Script HiFiGAN\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7059a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    import librosa\n",
    "except:\n",
    "    !pip install librosa\n",
    "try: \n",
    "    import optuna, plotly\n",
    "except:\n",
    "    !pip install optuna\n",
    "    !pip install plotly\n",
    "\n",
    "#Set Dir \n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.nn.utils import weight_norm \n",
    "from torch.nn import utils\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchaudio.transforms as T\n",
    "import optuna, plotly\n",
    "from optuna.importance import get_param_importances\n",
    "from optuna.visualization import plot_param_importances\n",
    "\n",
    "# Utils\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import logging, librosa, itertools, tensorboard\n",
    "from typing import Sequence, Optional, Callable\n",
    "\n",
    "\n",
    "# Base Scripts\n",
    "from Libraries.Utils import *\n",
    "from MainScripts.Conf import conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9d7fbc",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f12e407",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c14554",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_kernel: bool = True #Set to true if using a remote Kernel changes the file structure\n",
    "model_name: str = \"HiFiGAN_v2\"\n",
    "training_data_name: str = \"training_full_mel\"\n",
    "training_label_name: str = \"training_full_wave\"\n",
    "full_model_path: str = path_to_remote_path(\"{}/{}\".format(conf[\"paths\"].model_path, model_name + \".pth\"), remote_kernel)\n",
    "sw = SummaryWriter(path_to_remote_path(\"{}/{}\".format(conf[\"paths\"].model_path, 'logs'), remote_kernel))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f7c208",
   "metadata": {},
   "source": [
    "Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c656f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_level: int = LIGHT_DEBUG\n",
    "logging.basicConfig(level=logging_level, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger: logging.Logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b924858d",
   "metadata": {},
   "source": [
    "Training Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecc9b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_training_samples: int = 2496 // 2\n",
    "batch_size: int = 16\n",
    "tensor_wave_dim: list = [batch_size, 1, 2**17] #B, C, H = Batch, channels, Time domain\n",
    "tensor_mel_dim: list = [batch_size, 96, 512]\n",
    "learning_rate: float = 2e-4\n",
    "b1, b2 = [0.8, 0.99]\n",
    "epochs: int = 300\n",
    "restart_training: bool = True\n",
    "checkpoint_freq: int = 5\n",
    "num_workers: int = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571b189",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c201a4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:05:06,166 - LIGHT_DEBUG - Ndarray loaded from Data/training_full_mel.npy of shape: (6867, 96, 512)\n",
      "2025-06-18 15:05:14,003 - LIGHT_DEBUG - Ndarray loaded from Data/training_full_wave.npy of shape: (6867, 131072)\n"
     ]
    }
   ],
   "source": [
    "mel_data: ndarray = load_training_data(path_to_remote_path(\"{}/{}\".format(conf[\"paths\"].data_path, training_data_name + \".npy\"), remote_kernel))\n",
    "audio_data: ndarray = load_training_data(path_to_remote_path(\"{}/{}\".format(conf[\"paths\"].data_path, training_label_name + \".npy\"), remote_kernel))\n",
    "\n",
    "np.random.seed(50)\n",
    "indicies: ndarray = np.arange(mel_data.shape[0])\n",
    "np.random.shuffle(indicies)\n",
    "mel_data = mel_data[indicies]\n",
    "audio_data = audio_data[indicies]\n",
    "\n",
    "data_loader = create_dataloader(Audio_Data(mel_data[:n_training_samples], audio_data[:n_training_samples]), batch_size, num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63a34ad",
   "metadata": {},
   "source": [
    "### Original Impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8037dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m, mean=0.0, std=0.01):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        m.weight.data.normal_(mean, std)\n",
    "\n",
    "\n",
    "def apply_weight_norm(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        weight_norm(m)\n",
    "\n",
    "\n",
    "def get_padding(kernel_size, dilation=1):\n",
    "    return int((kernel_size*dilation - dilation)/2)\n",
    "\n",
    "\n",
    "class ResBlock1(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
    "        super(ResBlock1, self).__init__()\n",
    "        self.convs1 = nn.ModuleList([\n",
    "            weight_norm(nn.Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
    "                                padding=get_padding(kernel_size, dilation[0]))),\n",
    "            weight_norm(nn.Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
    "                                padding=get_padding(kernel_size, dilation[1]))),\n",
    "            weight_norm(nn.Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n",
    "                                padding=get_padding(kernel_size, dilation[2])))\n",
    "        ])\n",
    "        self.convs1.apply(init_weights)\n",
    "\n",
    "        self.convs2 = nn.ModuleList([\n",
    "            weight_norm(nn.Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                                padding=get_padding(kernel_size, 1))),\n",
    "            weight_norm(nn.Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                                padding=get_padding(kernel_size, 1))),\n",
    "            weight_norm(nn.Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                                padding=get_padding(kernel_size, 1)))\n",
    "        ])\n",
    "        self.convs2.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for c1, c2 in zip(self.convs1, self.convs2):\n",
    "            xt = F.leaky_relu(x, 0.1)\n",
    "            xt = c1(xt)\n",
    "            xt = F.leaky_relu(xt, 0.1)\n",
    "            xt = c2(xt)\n",
    "            x = xt + x\n",
    "        return x\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        for l in self.convs1:\n",
    "            utils.remove_weight_norm(l)\n",
    "        for l in self.convs2:\n",
    "            utils.remove_weight_norm(l)\n",
    "\n",
    "\n",
    "class ResBlock2(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3, dilation=(1, 3)):\n",
    "        super(ResBlock2, self).__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            weight_norm(nn.Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
    "                                padding=get_padding(kernel_size, dilation[0]))),\n",
    "            weight_norm(nn.Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
    "                                padding=get_padding(kernel_size, dilation[1])))\n",
    "        ])\n",
    "        self.convs.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for c in self.convs:\n",
    "            xt = F.leaky_relu(x, 0.1)\n",
    "            xt = c(xt)\n",
    "            x = xt + x\n",
    "        return x\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        for l in self.convs:\n",
    "            utils.remove_weight_norm(l)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, n_mel_channels: int, resblock_kernel_sizes: list[int], upsample_rates: list[int], upsample_initial_channel: int, upsample_kernel_sizes: list[int], resblock_dilation_sizes: list[int], resblock: int = 1):\n",
    "        super(Generator, self).__init__()\n",
    "        self.num_kernels = len(resblock_kernel_sizes)\n",
    "        self.num_upsamples = len(upsample_rates)\n",
    "        self.conv_pre = weight_norm(nn.Conv1d(n_mel_channels, upsample_initial_channel, 7, 1, padding=3))\n",
    "        resblock = ResBlock1 if resblock == 1 else ResBlock2\n",
    "\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n",
    "            self.ups.append(weight_norm(\n",
    "                nn.ConvTranspose1d(upsample_initial_channel//(2**i), upsample_initial_channel//(2**(i+1)),\n",
    "                                k, u, padding=(k-u)//2)))\n",
    "\n",
    "        self.resblocks = nn.ModuleList()\n",
    "        for i in range(len(self.ups)):\n",
    "            ch = upsample_initial_channel//(2**(i+1))\n",
    "            for j, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):\n",
    "                self.resblocks.append(resblock(ch, k, d))\n",
    "\n",
    "        self.conv_post = weight_norm(nn.Conv1d(ch, 1, 7, 1, padding=3))\n",
    "        self.ups.apply(init_weights)\n",
    "        self.conv_post.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_pre(x)\n",
    "        for i in range(self.num_upsamples):\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            x = self.ups[i](x)\n",
    "            xs = None\n",
    "            for j in range(self.num_kernels):\n",
    "                if xs is None:\n",
    "                    xs = self.resblocks[i*self.num_kernels+j](x)\n",
    "                else:\n",
    "                    xs += self.resblocks[i*self.num_kernels+j](x)\n",
    "            x = xs / self.num_kernels\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv_post(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        logger.light_debug('Removing weight norm...')\n",
    "        for l in self.ups:\n",
    "            utils.remove_weight_norm(l)\n",
    "        for l in self.resblocks:\n",
    "            l.remove_weight_norm()\n",
    "        utils.remove_weight_norm(self.conv_pre)\n",
    "        utils.remove_weight_norm(self.conv_post)\n",
    "\n",
    "\n",
    "class DiscriminatorP(nn.Module):\n",
    "    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n",
    "        super(DiscriminatorP, self).__init__()\n",
    "        self.period = period\n",
    "        norm_f = weight_norm if use_spectral_norm == False else utils.spectral_norm\n",
    "        self.convs = nn.ModuleList([\n",
    "            norm_f(nn.Conv2d(1, 32, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n",
    "            norm_f(nn.Conv2d(32, 128, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n",
    "            norm_f(nn.Conv2d(128, 512, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n",
    "            norm_f(nn.Conv2d(512, 1024, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n",
    "            norm_f(nn.Conv2d(1024, 1024, (kernel_size, 1), 1, padding=(2, 0))),\n",
    "        ])\n",
    "        self.conv_post = norm_f(nn.Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmap = []\n",
    "\n",
    "        # 1d to 2d\n",
    "        b, c, t = x.shape\n",
    "        if t % self.period != 0: # pad first\n",
    "            n_pad = self.period - (t % self.period)\n",
    "            x = F.pad(x, (0, n_pad), \"reflect\")\n",
    "            t = t + n_pad\n",
    "        x = x.view(b, c, t // self.period, self.period)\n",
    "\n",
    "        for l in self.convs:\n",
    "            x = l(x)\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            fmap.append(x)\n",
    "        x = self.conv_post(x)\n",
    "        fmap.append(x)\n",
    "        x = torch.flatten(x, 1, -1)\n",
    "\n",
    "        return x, fmap\n",
    "\n",
    "\n",
    "class MultiPeriodDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiPeriodDiscriminator, self).__init__()\n",
    "        self.discriminators = nn.ModuleList([\n",
    "            DiscriminatorP(2),\n",
    "            DiscriminatorP(3),\n",
    "            DiscriminatorP(5),\n",
    "            DiscriminatorP(7),\n",
    "            DiscriminatorP(11),\n",
    "        ])\n",
    "\n",
    "    def forward(self, y, y_hat):\n",
    "        y_d_rs = []\n",
    "        y_d_gs = []\n",
    "        fmap_rs = []\n",
    "        fmap_gs = []\n",
    "        for i, d in enumerate(self.discriminators):\n",
    "            y_d_r, fmap_r = d(y)\n",
    "            y_d_g, fmap_g = d(y_hat)\n",
    "            y_d_rs.append(y_d_r)\n",
    "            fmap_rs.append(fmap_r)\n",
    "            y_d_gs.append(y_d_g)\n",
    "            fmap_gs.append(fmap_g)\n",
    "\n",
    "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n",
    "\n",
    "\n",
    "class DiscriminatorS(torch.nn.Module):\n",
    "    def __init__(self, use_spectral_norm=False):\n",
    "        super(DiscriminatorS, self).__init__()\n",
    "        norm_f = weight_norm if use_spectral_norm == False else utils.spectral_norm\n",
    "        self.convs = nn.ModuleList([\n",
    "            norm_f(nn.Conv1d(1, 128, 15, 1, padding=7)),\n",
    "            norm_f(nn.Conv1d(128, 128, 41, 2, groups=4, padding=20)),\n",
    "            norm_f(nn.Conv1d(128, 256, 41, 2, groups=16, padding=20)),\n",
    "            norm_f(nn.Conv1d(256, 512, 41, 4, groups=16, padding=20)),\n",
    "            norm_f(nn.Conv1d(512, 1024, 41, 4, groups=16, padding=20)),\n",
    "            norm_f(nn.Conv1d(1024, 1024, 41, 1, groups=16, padding=20)),\n",
    "            norm_f(nn.Conv1d(1024, 1024, 5, 1, padding=2)),\n",
    "        ])\n",
    "        self.conv_post = norm_f(nn.Conv1d(1024, 1, 3, 1, padding=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmap = []\n",
    "        for l in self.convs:\n",
    "            x = l(x)\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            fmap.append(x)\n",
    "        x = self.conv_post(x)\n",
    "        fmap.append(x)\n",
    "        x = torch.flatten(x, 1, -1)\n",
    "\n",
    "        return x, fmap\n",
    "\n",
    "\n",
    "class MultiScaleDiscriminator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiScaleDiscriminator, self).__init__()\n",
    "        self.discriminators = nn.ModuleList([\n",
    "            DiscriminatorS(use_spectral_norm=True),\n",
    "            DiscriminatorS(),\n",
    "            DiscriminatorS(),\n",
    "        ])\n",
    "        self.meanpools = nn.ModuleList([\n",
    "            nn.AvgPool1d(4, 2, padding=2),\n",
    "            nn.AvgPool1d(4, 2, padding=2)\n",
    "        ])\n",
    "\n",
    "    def forward(self, y, y_hat):\n",
    "        y_d_rs = []\n",
    "        y_d_gs = []\n",
    "        fmap_rs = []\n",
    "        fmap_gs = []\n",
    "        for i, d in enumerate(self.discriminators):\n",
    "            if i != 0:\n",
    "                y = self.meanpools[i-1](y)\n",
    "                y_hat = self.meanpools[i-1](y_hat)\n",
    "            y_d_r, fmap_r = d(y)\n",
    "            y_d_g, fmap_g = d(y_hat)\n",
    "            y_d_rs.append(y_d_r)\n",
    "            fmap_rs.append(fmap_r)\n",
    "            y_d_gs.append(y_d_g)\n",
    "            fmap_gs.append(fmap_g)\n",
    "\n",
    "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n",
    "\n",
    "\n",
    "def feature_loss(fmap_r, fmap_g):\n",
    "    loss = 0\n",
    "    for dr, dg in zip(fmap_r, fmap_g):\n",
    "        for rl, gl in zip(dr, dg):\n",
    "            loss += torch.mean(torch.abs(rl - gl))\n",
    "\n",
    "    return loss*2\n",
    "\n",
    "\n",
    "def discriminator_loss(disc_real_outputs, disc_generated_outputs):\n",
    "    loss = 0\n",
    "    r_losses = []\n",
    "    g_losses = []\n",
    "    for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
    "        r_loss = torch.mean((1-dr)**2)\n",
    "        g_loss = torch.mean(dg**2)\n",
    "        loss += (r_loss + g_loss)\n",
    "        r_losses.append(r_loss.item())\n",
    "        g_losses.append(g_loss.item())\n",
    "\n",
    "    return loss, r_losses, g_losses\n",
    "\n",
    "\n",
    "def generator_loss(disc_outputs):\n",
    "    loss = 0\n",
    "    gen_losses = []\n",
    "    for dg in disc_outputs:\n",
    "        l = torch.mean((1-dg)**2)\n",
    "        gen_losses.append(l)\n",
    "        loss += l\n",
    "\n",
    "    return loss, gen_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e6de654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "generator = Generator(\n",
    "                    n_mel_channels=96,\n",
    "                    upsample_rates=[8,8,2,2],\n",
    "                    upsample_kernel_sizes=[16,16,4,4],\n",
    "                    upsample_initial_channel=512,\n",
    "                    resblock_kernel_sizes=[3,7,11],\n",
    "                    resblock_dilation_sizes=[[1,3,5], [1,3,5], [1,3,5]],\n",
    "                    resblock=1\n",
    "                ).to(device)\n",
    "mpd = MultiPeriodDiscriminator().to(device)\n",
    "msd = MultiScaleDiscriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7197e270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:05:31,544 - INFO - Model HiFiGAN_v2 loaded with ~13.99M G and ~41.10M, ~29.61M D Parameters\n"
     ]
    }
   ],
   "source": [
    "optim_g = torch.optim.AdamW(generator.parameters(), learning_rate, betas=[b1, b2])\n",
    "optim_d = torch.optim.AdamW(itertools.chain(msd.parameters(), mpd.parameters()),\n",
    "                                5e-5, betas=[b1, b2])\n",
    "gen_lr_scheduler = optim.lr_scheduler.ExponentialLR(optim_g, gamma=0.999)\n",
    "disc_lr_scheduler = optim.lr_scheduler.ExponentialLR(optim_d, gamma=0.999)\n",
    "start_epoch: int = 0\n",
    "if os.path.exists(full_model_path):\n",
    "    model = torch.load(full_model_path, map_location=device)\n",
    "    generator.load_state_dict(model[\"generator\"])\n",
    "    msd.load_state_dict(model[\"msd\"])\n",
    "    mpd.load_state_dict(model[\"mpd\"])\n",
    "    if not restart_training:\n",
    "        optim_g.load_state_dict(model[\"optim_g\"])\n",
    "        optim_d.load_state_dict(model[\"optim_d\"])\n",
    "        start_epoch = model.get(\"epoch\", 0)\n",
    "    logger.info(f\"Model {model_name} loaded with {count_parameters(generator)} G and {count_parameters(mpd)}, {count_parameters(msd)} D Parameters\")\n",
    "else: \n",
    "    logger.info(f\"Model {model_name} created with {count_parameters(generator)} G and {count_parameters(mpd)}, {count_parameters(msd)} D Parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a3f63a",
   "metadata": {},
   "source": [
    "Load universal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b4f5c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc = torch.load(path_to_remote_path(\"../Models/disc_universal\", remote_kernel), map_location=device)\n",
    "gen = torch.load(path_to_remote_path(\"../Models/gen_universal\", remote_kernel), map_location=device)\n",
    "#pretrained_dict = gen[\"generator\"]\n",
    "#model_state_dict = generator.state_dict()\n",
    "#new_sd = {k: v for k, v in pretrained_dict.items() if k not in [\"conv_pre.weight_v\", \"conv_pre.weight_g\", \"conv_pre.bias\"] and v.size() == #model_state_dict.get(k, torch.Size([0])).size()}\n",
    "#pretrained_weight = pretrained_dict[\"conv_pre.weight_v\"]  # Shape: [512, 80, 7]\n",
    "#padded_weight = torch.zeros([512, 96, 7], device=device)\n",
    "#padded_weight[:, :80, :] = pretrained_weight  # Copy the first 80 channels\n",
    "#padded_weight[:, 80:, :] = torch.randn([512, 16, 7], device=device) * 0.01  # Initialize the rest\n",
    "#model_state_dict[\"conv_pre.weight_v\"] = padded_weight\n",
    "\n",
    "#generator.load_state_dict(new_sd, strict=False)\n",
    "\n",
    "#generator.load_state_dict(model_state_dict, strict=False)\n",
    "\n",
    "msd.load_state_dict(disc[\"msd\"])\n",
    "mpd.load_state_dict(disc[\"mpd\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f9b055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0336287b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 08:32:42,318 - INFO - Training started on cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 08:38:09,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.542 67.4383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 08:38:10,212 - INFO - Epoch 002: Avg. D/G Loss: 9.4092e-01, 7.0011e+01 Avg. reconst. Loss: 1.7105e-01 Remaining Time: 13h 34min 15s LR: 4.99001e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 08:40:43,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.988 68.187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 08:40:44,014 - INFO - Epoch 003: Avg. D/G Loss: 7.5049e-01, 7.0193e+01 Avg. reconst. Loss: 1.7203e-01 Remaining Time: 13h 14min 47s LR: 4.98501e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 08:43:17,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.474 73.417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 08:43:17,830 - INFO - Epoch 004: Avg. D/G Loss: 6.5527e-01, 7.1277e+01 Avg. reconst. Loss: 1.7368e-01 Remaining Time: 13h 03min 47s LR: 4.98003e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 08:45:51,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.272 71.065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 08:45:51,440 - INFO - Epoch 005: Avg. D/G Loss: 4.6143e-01, 7.4756e+01 Avg. reconst. Loss: 1.7745e-01 Remaining Time: 12h 55min 57s LR: 4.97505e-05 \n",
      "2025-06-18 08:45:52,444 - LIGHT_DEBUG - Checkpoint saved model to Models/HiFiGAN_v2_epoch_005.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 08:48:25,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.209 71.327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 08:48:25,693 - INFO - Epoch 006: Avg. D/G Loss: 4.3799e-01, 7.2847e+01 Avg. reconst. Loss: 1.7844e-01 Remaining Time: 12h 49min 35s LR: 4.97007e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 08:50:58,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.215 71.845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 08:50:59,005 - INFO - Epoch 007: Avg. D/G Loss: 2.3218e-01, 7.3030e+01 Avg. reconst. Loss: 1.7578e-01 Remaining Time: 12h 44min 21s LR: 4.96510e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 08:53:31,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.197 79.209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 08:53:32,316 - INFO - Epoch 008: Avg. D/G Loss: 3.5327e-01, 7.4318e+01 Avg. reconst. Loss: 1.7802e-01 Remaining Time: 12h 39min 47s LR: 4.96014e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 08:56:05,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.536 75.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 08:56:05,620 - INFO - Epoch 009: Avg. D/G Loss: 2.6953e-01, 7.3114e+01 Avg. reconst. Loss: 1.7511e-01 Remaining Time: 12h 35min 40s LR: 4.95518e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 08:58:52,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.289 69.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 08:58:53,198 - INFO - Epoch 010: Avg. D/G Loss: 2.2913e-01, 7.3172e+01 Avg. reconst. Loss: 1.8116e-01 Remaining Time: 12h 38min 46s LR: 4.95022e-05 \n",
      "2025-06-18 08:58:54,221 - LIGHT_DEBUG - Models/HiFiGAN_v2_epoch_005.pth deleted\n",
      "2025-06-18 08:58:54,221 - LIGHT_DEBUG - Checkpoint saved model to Models/HiFiGAN_v2_epoch_010.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:02:14,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.273 77.883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:02:15,268 - INFO - Epoch 011: Avg. D/G Loss: 2.1753e-01, 7.2897e+01 Avg. reconst. Loss: 1.7274e-01 Remaining Time: 12h 55min 26s LR: 4.94527e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:05:36,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.090 78.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:05:36,489 - INFO - Epoch 012: Avg. D/G Loss: 1.6821e-01, 7.4154e+01 Avg. reconst. Loss: 1.7502e-01 Remaining Time: 13h 08min 51s LR: 4.94033e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:08:57,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.106 72.149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:08:57,896 - INFO - Epoch 013: Avg. D/G Loss: 1.0815e-01, 7.2610e+01 Avg. reconst. Loss: 1.7375e-01 Remaining Time: 13h 19min 45s LR: 4.93539e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:12:18,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.104 80.255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:12:19,307 - INFO - Epoch 014: Avg. D/G Loss: 3.3325e-01, 7.3864e+01 Avg. reconst. Loss: 1.7702e-01 Remaining Time: 13h 28min 36s LR: 4.93045e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:15:40,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.098 67.079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:15:40,767 - INFO - Epoch 015: Avg. D/G Loss: 1.6150e-01, 7.1848e+01 Avg. reconst. Loss: 1.7191e-01 Remaining Time: 13h 35min 51s LR: 4.92552e-05 \n",
      "2025-06-18 09:15:41,717 - LIGHT_DEBUG - Models/HiFiGAN_v2_epoch_010.pth deleted\n",
      "2025-06-18 09:15:41,718 - LIGHT_DEBUG - Checkpoint saved model to Models/HiFiGAN_v2_epoch_015.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:19:02,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.082 69.273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:19:03,171 - INFO - Epoch 016: Avg. D/G Loss: 1.1981e-01, 7.1117e+01 Avg. reconst. Loss: 1.7003e-01 Remaining Time: 13h 41min 46s LR: 4.92060e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:22:24,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.120 78.922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:22:24,607 - INFO - Epoch 017: Avg. D/G Loss: 1.4099e-01, 7.3035e+01 Avg. reconst. Loss: 1.7320e-01 Remaining Time: 13h 46min 36s LR: 4.91568e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:25:45,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.215 67.972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:25:46,065 - INFO - Epoch 018: Avg. D/G Loss: 2.6318e-01, 7.2734e+01 Avg. reconst. Loss: 1.7399e-01 Remaining Time: 13h 50min 31s LR: 4.91076e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:32:28,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.060 72.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:32:28,978 - INFO - Epoch 020: Avg. D/G Loss: 1.4148e-01, 7.2874e+01 Avg. reconst. Loss: 1.7314e-01 Remaining Time: 13h 56min 11s LR: 4.90094e-05 \n",
      "2025-06-18 09:32:30,260 - LIGHT_DEBUG - Models/HiFiGAN_v2_epoch_015.pth deleted\n",
      "2025-06-18 09:32:30,260 - LIGHT_DEBUG - Checkpoint saved model to Models/HiFiGAN_v2_epoch_020.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:35:51,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.124 67.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:35:51,732 - INFO - Epoch 021: Avg. D/G Loss: 8.5144e-02, 7.0816e+01 Avg. reconst. Loss: 1.6961e-01 Remaining Time: 13h 58min 08s LR: 4.89604e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:39:12,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.044 72.970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:39:13,150 - INFO - Epoch 022: Avg. D/G Loss: 7.1106e-02, 7.0985e+01 Avg. reconst. Loss: 1.6982e-01 Remaining Time: 13h 59min 35s LR: 4.89115e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:42:34,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.498 73.267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:42:34,686 - INFO - Epoch 023: Avg. D/G Loss: 2.6367e-01, 7.4121e+01 Avg. reconst. Loss: 1.7736e-01 Remaining Time: 14h 00min 39s LR: 4.88626e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:45:55,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.276 70.553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:45:56,290 - INFO - Epoch 024: Avg. D/G Loss: 2.7441e-01, 7.2530e+01 Avg. reconst. Loss: 1.7041e-01 Remaining Time: 14h 01min 21s LR: 4.88137e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:49:17,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.061 75.760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:49:17,730 - INFO - Epoch 025: Avg. D/G Loss: 1.1987e-01, 7.3167e+01 Avg. reconst. Loss: 1.7562e-01 Remaining Time: 14h 01min 42s LR: 4.87649e-05 \n",
      "2025-06-18 09:49:18,628 - LIGHT_DEBUG - Models/HiFiGAN_v2_epoch_020.pth deleted\n",
      "2025-06-18 09:49:18,629 - LIGHT_DEBUG - Checkpoint saved model to Models/HiFiGAN_v2_epoch_025.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:52:39,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.044 72.862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:52:40,012 - INFO - Epoch 026: Avg. D/G Loss: 7.3425e-02, 7.1948e+01 Avg. reconst. Loss: 1.7236e-01 Remaining Time: 14h 01min 45s LR: 4.87161e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:56:01,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.074 72.483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:56:01,440 - INFO - Epoch 027: Avg. D/G Loss: 1.1688e-01, 7.2754e+01 Avg. reconst. Loss: 1.7328e-01 Remaining Time: 14h 01min 34s LR: 4.86674e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:59:22,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.127 78.774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 09:59:22,826 - INFO - Epoch 028: Avg. D/G Loss: 1.4722e-01, 7.3720e+01 Avg. reconst. Loss: 1.7431e-01 Remaining Time: 14h 01min 08s LR: 4.86187e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:02:43,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.128 81.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:02:44,356 - INFO - Epoch 029: Avg. D/G Loss: 1.2598e-01, 7.3592e+01 Avg. reconst. Loss: 1.7210e-01 Remaining Time: 14h 00min 32s LR: 4.85701e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:06:05,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.066 78.539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:06:05,905 - INFO - Epoch 030: Avg. D/G Loss: 9.8267e-02, 7.4646e+01 Avg. reconst. Loss: 1.6835e-01 Remaining Time: 13h 59min 45s LR: 4.85215e-05 \n",
      "2025-06-18 10:06:06,907 - LIGHT_DEBUG - Models/HiFiGAN_v2_epoch_025.pth deleted\n",
      "2025-06-18 10:06:06,908 - LIGHT_DEBUG - Checkpoint saved model to Models/HiFiGAN_v2_epoch_030.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:09:27,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.044 66.3088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:09:28,285 - INFO - Epoch 031: Avg. D/G Loss: 1.8152e-01, 7.2772e+01 Avg. reconst. Loss: 1.7001e-01 Remaining Time: 13h 58min 47s LR: 4.84730e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:12:49,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.066 72.300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:12:49,700 - INFO - Epoch 032: Avg. D/G Loss: 5.9937e-02, 7.1775e+01 Avg. reconst. Loss: 1.7004e-01 Remaining Time: 13h 57min 39s LR: 4.84246e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:16:10,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.064 74.478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:16:11,135 - INFO - Epoch 033: Avg. D/G Loss: 9.3201e-02, 7.7152e+01 Avg. reconst. Loss: 1.7663e-01 Remaining Time: 13h 56min 24s LR: 4.83761e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:19:32,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.076 71.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:19:32,574 - INFO - Epoch 034: Avg. D/G Loss: 5.5298e-02, 7.2240e+01 Avg. reconst. Loss: 1.6889e-01 Remaining Time: 13h 55min 02s LR: 4.83278e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:22:53,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.132 73.859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:22:54,030 - INFO - Epoch 035: Avg. D/G Loss: 2.6929e-01, 7.8764e+01 Avg. reconst. Loss: 1.7977e-01 Remaining Time: 13h 53min 33s LR: 4.82794e-05 \n",
      "2025-06-18 10:22:55,009 - LIGHT_DEBUG - Models/HiFiGAN_v2_epoch_030.pth deleted\n",
      "2025-06-18 10:22:55,010 - LIGHT_DEBUG - Checkpoint saved model to Models/HiFiGAN_v2_epoch_035.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:26:15,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.051 74.661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:26:16,321 - INFO - Epoch 036: Avg. D/G Loss: 8.7097e-02, 7.3027e+01 Avg. reconst. Loss: 1.7352e-01 Remaining Time: 13h 51min 56s LR: 4.82311e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:29:37,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.093 72.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:29:37,798 - INFO - Epoch 037: Avg. D/G Loss: 7.5745e-02, 7.3578e+01 Avg. reconst. Loss: 1.7216e-01 Remaining Time: 13h 50min 15s LR: 4.81829e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:32:58,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.059 72.560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:32:59,254 - INFO - Epoch 038: Avg. D/G Loss: 1.1157e-01, 7.5485e+01 Avg. reconst. Loss: 1.7582e-01 Remaining Time: 13h 48min 29s LR: 4.81347e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:36:20,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.042 74.110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:36:20,668 - INFO - Epoch 039: Avg. D/G Loss: 5.5389e-02, 7.4310e+01 Avg. reconst. Loss: 1.7334e-01 Remaining Time: 13h 46min 37s LR: 4.80866e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:39:41,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.173 75.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:39:42,101 - INFO - Epoch 040: Avg. D/G Loss: 9.6375e-02, 7.4859e+01 Avg. reconst. Loss: 1.7281e-01 Remaining Time: 13h 44min 41s LR: 4.80385e-05 \n",
      "2025-06-18 10:39:42,964 - LIGHT_DEBUG - Models/HiFiGAN_v2_epoch_035.pth deleted\n",
      "2025-06-18 10:39:42,965 - LIGHT_DEBUG - Checkpoint saved model to Models/HiFiGAN_v2_epoch_040.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:43:03,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.064 78.049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:43:04,300 - INFO - Epoch 041: Avg. D/G Loss: 2.2314e-01, 7.1545e+01 Avg. reconst. Loss: 1.6914e-01 Remaining Time: 13h 42min 41s LR: 4.79905e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:46:25,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.040 75.036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:46:25,754 - INFO - Epoch 042: Avg. D/G Loss: 6.5552e-02, 7.2133e+01 Avg. reconst. Loss: 1.6853e-01 Remaining Time: 13h 40min 37s LR: 4.79425e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:49:46,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.069 73.371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:49:47,228 - INFO - Epoch 043: Avg. D/G Loss: 1.2732e-01, 7.3190e+01 Avg. reconst. Loss: 1.7318e-01 Remaining Time: 13h 38min 30s LR: 4.78945e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:53:08,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.099 68.603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:53:08,645 - INFO - Epoch 044: Avg. D/G Loss: 7.3059e-02, 7.2789e+01 Avg. reconst. Loss: 1.6918e-01 Remaining Time: 13h 36min 19s LR: 4.78466e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:56:29,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.045 77.235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:56:30,088 - INFO - Epoch 045: Avg. D/G Loss: 6.1310e-02, 7.3167e+01 Avg. reconst. Loss: 1.6942e-01 Remaining Time: 13h 34min 05s LR: 4.77988e-05 \n",
      "2025-06-18 10:56:31,223 - LIGHT_DEBUG - Models/HiFiGAN_v2_epoch_040.pth deleted\n",
      "2025-06-18 10:56:31,224 - LIGHT_DEBUG - Checkpoint saved model to Models/HiFiGAN_v2_epoch_045.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:59:52,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.042 73.199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 10:59:52,712 - INFO - Epoch 046: Avg. D/G Loss: 1.8250e-01, 7.6660e+01 Avg. reconst. Loss: 1.8044e-01 Remaining Time: 13h 31min 48s LR: 4.77510e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:03:13,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.096 72.092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:03:14,220 - INFO - Epoch 047: Avg. D/G Loss: 6.9519e-02, 7.5233e+01 Avg. reconst. Loss: 1.8313e-01 Remaining Time: 13h 29min 29s LR: 4.77032e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:06:35,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.190 76.341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:06:35,699 - INFO - Epoch 048: Avg. D/G Loss: 2.0935e-01, 7.3656e+01 Avg. reconst. Loss: 1.7613e-01 Remaining Time: 13h 27min 07s LR: 4.76555e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:09:56,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.078 74.901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:09:57,189 - INFO - Epoch 049: Avg. D/G Loss: 8.7830e-02, 7.4016e+01 Avg. reconst. Loss: 1.7305e-01 Remaining Time: 13h 24min 42s LR: 4.76079e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:13:18,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.083 76.856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:13:18,676 - INFO - Epoch 050: Avg. D/G Loss: 5.8990e-02, 7.5152e+01 Avg. reconst. Loss: 1.7386e-01 Remaining Time: 13h 22min 15s LR: 4.75603e-05 \n",
      "2025-06-18 11:13:19,637 - LIGHT_DEBUG - Models/HiFiGAN_v2_epoch_045.pth deleted\n",
      "2025-06-18 11:13:19,638 - LIGHT_DEBUG - Checkpoint saved model to Models/HiFiGAN_v2_epoch_050.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:16:40,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.047 78.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:16:41,127 - INFO - Epoch 051: Avg. D/G Loss: 6.4758e-02, 7.5110e+01 Avg. reconst. Loss: 1.7215e-01 Remaining Time: 13h 19min 46s LR: 4.75127e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:20:02,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.031 76.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:20:02,654 - INFO - Epoch 052: Avg. D/G Loss: 5.2826e-02, 7.6137e+01 Avg. reconst. Loss: 1.7461e-01 Remaining Time: 13h 17min 16s LR: 4.74652e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:23:23,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.044 80.627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:23:24,182 - INFO - Epoch 053: Avg. D/G Loss: 1.4514e-01, 7.7237e+01 Avg. reconst. Loss: 1.8031e-01 Remaining Time: 13h 14min 43s LR: 4.74177e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:26:45,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.203 79.018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:26:45,666 - INFO - Epoch 054: Avg. D/G Loss: 2.0276e-01, 7.7497e+01 Avg. reconst. Loss: 1.7771e-01 Remaining Time: 13h 12min 08s LR: 4.73703e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:30:06,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.068 71.666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:30:07,208 - INFO - Epoch 055: Avg. D/G Loss: 9.4727e-02, 7.6173e+01 Avg. reconst. Loss: 1.7387e-01 Remaining Time: 13h 09min 32s LR: 4.73230e-05 \n",
      "2025-06-18 11:30:08,208 - LIGHT_DEBUG - Models/HiFiGAN_v2_epoch_050.pth deleted\n",
      "2025-06-18 11:30:08,209 - LIGHT_DEBUG - Checkpoint saved model to Models/HiFiGAN_v2_epoch_055.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:33:29,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.054 75.062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:33:29,694 - INFO - Epoch 056: Avg. D/G Loss: 5.6000e-02, 7.3602e+01 Avg. reconst. Loss: 1.6852e-01 Remaining Time: 13h 06min 54s LR: 4.72756e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:36:50,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.172 86.373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:36:51,229 - INFO - Epoch 057: Avg. D/G Loss: 1.2341e-01, 7.4771e+01 Avg. reconst. Loss: 1.7027e-01 Remaining Time: 13h 04min 15s LR: 4.72284e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:40:12,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.083 77.402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:40:12,739 - INFO - Epoch 058: Avg. D/G Loss: 7.6843e-02, 7.7053e+01 Avg. reconst. Loss: 1.7519e-01 Remaining Time: 13h 01min 34s LR: 4.71811e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:43:33,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.049 73.136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:43:34,198 - INFO - Epoch 059: Avg. D/G Loss: 4.6326e-02, 7.6358e+01 Avg. reconst. Loss: 1.7199e-01 Remaining Time: 12h 58min 52s LR: 4.71339e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:46:55,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.059 76.988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:46:55,726 - INFO - Epoch 060: Avg. D/G Loss: 5.9509e-02, 7.7394e+01 Avg. reconst. Loss: 1.7351e-01 Remaining Time: 12h 56min 08s LR: 4.70868e-05 \n",
      "2025-06-18 11:46:56,854 - LIGHT_DEBUG - Models/HiFiGAN_v2_epoch_055.pth deleted\n",
      "2025-06-18 11:46:56,855 - LIGHT_DEBUG - Checkpoint saved model to Models/HiFiGAN_v2_epoch_060.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:50:17,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.113 79.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:50:18,321 - INFO - Epoch 061: Avg. D/G Loss: 2.0752e-01, 7.8373e+01 Avg. reconst. Loss: 1.7527e-01 Remaining Time: 12h 53min 24s LR: 4.70397e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:53:39,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.107 77.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:53:39,834 - INFO - Epoch 062: Avg. D/G Loss: 7.5745e-02, 7.7464e+01 Avg. reconst. Loss: 1.7393e-01 Remaining Time: 12h 50min 38s LR: 4.69927e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:57:00,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.052 77.047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:57:01,291 - INFO - Epoch 063: Avg. D/G Loss: 6.2195e-02, 7.5834e+01 Avg. reconst. Loss: 1.6933e-01 Remaining Time: 12h 47min 50s LR: 4.69457e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:00:22,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.029 68.007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:00:22,813 - INFO - Epoch 064: Avg. D/G Loss: 5.2979e-02, 7.6005e+01 Avg. reconst. Loss: 1.6950e-01 Remaining Time: 12h 45min 02s LR: 4.68987e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:03:43,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.314 75.7986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:03:44,304 - INFO - Epoch 065: Avg. D/G Loss: 1.8262e-01, 7.6583e+01 Avg. reconst. Loss: 1.7073e-01 Remaining Time: 12h 42min 13s LR: 4.68518e-05 \n",
      "2025-06-18 12:03:45,218 - LIGHT_DEBUG - Models/HiFiGAN_v2_epoch_060.pth deleted\n",
      "2025-06-18 12:03:45,219 - LIGHT_DEBUG - Checkpoint saved model to Models/HiFiGAN_v2_epoch_065.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:07:06,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.040 77.574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:07:06,674 - INFO - Epoch 066: Avg. D/G Loss: 1.1206e-01, 7.9383e+01 Avg. reconst. Loss: 1.7655e-01 Remaining Time: 12h 39min 23s LR: 4.68050e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:10:27,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.025 80.167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:10:28,176 - INFO - Epoch 067: Avg. D/G Loss: 4.6295e-02, 7.7360e+01 Avg. reconst. Loss: 1.6975e-01 Remaining Time: 12h 36min 32s LR: 4.67582e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:13:49,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.050 80.132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:13:49,603 - INFO - Epoch 068: Avg. D/G Loss: 9.3567e-02, 8.1976e+01 Avg. reconst. Loss: 1.7953e-01 Remaining Time: 12h 33min 39s LR: 4.67114e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:17:10,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.040 71.187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:17:11,091 - INFO - Epoch 069: Avg. D/G Loss: 1.6321e-01, 7.8424e+01 Avg. reconst. Loss: 1.7332e-01 Remaining Time: 12h 30min 46s LR: 4.66647e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:20:32,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.047 73.679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:20:32,614 - INFO - Epoch 070: Avg. D/G Loss: 5.5481e-02, 7.5830e+01 Avg. reconst. Loss: 1.6897e-01 Remaining Time: 12h 27min 53s LR: 4.66181e-05 \n",
      "2025-06-18 12:20:33,582 - LIGHT_DEBUG - Models/HiFiGAN_v2_epoch_065.pth deleted\n",
      "2025-06-18 12:20:33,582 - LIGHT_DEBUG - Checkpoint saved model to Models/HiFiGAN_v2_epoch_070.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:23:54,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.049 76.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:23:55,065 - INFO - Epoch 071: Avg. D/G Loss: 9.9365e-02, 7.9740e+01 Avg. reconst. Loss: 1.7420e-01 Remaining Time: 12h 24min 58s LR: 4.65714e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:27:16,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.045 79.514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:27:16,535 - INFO - Epoch 072: Avg. D/G Loss: 4.8248e-02, 8.0555e+01 Avg. reconst. Loss: 1.7517e-01 Remaining Time: 12h 22min 03s LR: 4.65249e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:30:37,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.432 95.956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:30:38,011 - INFO - Epoch 073: Avg. D/G Loss: 7.0984e-02, 8.0133e+01 Avg. reconst. Loss: 1.7208e-01 Remaining Time: 12h 19min 07s LR: 4.64783e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:33:59,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.182 81.840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:33:59,503 - INFO - Epoch 074: Avg. D/G Loss: 1.6125e-01, 8.2647e+01 Avg. reconst. Loss: 1.7824e-01 Remaining Time: 12h 16min 10s LR: 4.64319e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:37:20,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.067 76.601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:37:21,019 - INFO - Epoch 075: Avg. D/G Loss: 7.6782e-02, 7.6962e+01 Avg. reconst. Loss: 1.6938e-01 Remaining Time: 12h 13min 13s LR: 4.63854e-05 \n",
      "2025-06-18 12:37:22,156 - LIGHT_DEBUG - Models/HiFiGAN_v2_epoch_070.pth deleted\n",
      "2025-06-18 12:37:22,157 - LIGHT_DEBUG - Checkpoint saved model to Models/HiFiGAN_v2_epoch_075.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:40:43,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.043 76.294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:40:43,645 - INFO - Epoch 076: Avg. D/G Loss: 7.5134e-02, 7.9897e+01 Avg. reconst. Loss: 1.7232e-01 Remaining Time: 12h 10min 15s LR: 4.63390e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:44:04,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.046 79.884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:44:05,218 - INFO - Epoch 077: Avg. D/G Loss: 1.6150e-01, 8.0789e+01 Avg. reconst. Loss: 1.7477e-01 Remaining Time: 12h 07min 17s LR: 4.62927e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:47:26,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.031 78.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:47:26,844 - INFO - Epoch 078: Avg. D/G Loss: 5.1483e-02, 7.9108e+01 Avg. reconst. Loss: 1.6924e-01 Remaining Time: 12h 04min 18s LR: 4.62464e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:50:47,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.038 79.566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:50:48,407 - INFO - Epoch 079: Avg. D/G Loss: 1.0150e-01, 8.0810e+01 Avg. reconst. Loss: 1.7198e-01 Remaining Time: 12h 01min 18s LR: 4.62002e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:54:09,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.034 78.290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:54:09,974 - INFO - Epoch 080: Avg. D/G Loss: 4.0863e-02, 8.0147e+01 Avg. reconst. Loss: 1.6985e-01 Remaining Time: 11h 58min 18s LR: 4.61540e-05 \n",
      "2025-06-18 12:54:10,920 - LIGHT_DEBUG - Models/HiFiGAN_v2_epoch_075.pth deleted\n",
      "2025-06-18 12:54:10,921 - LIGHT_DEBUG - Checkpoint saved model to Models/HiFiGAN_v2_epoch_080.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:57:31,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.072 82.631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 12:57:32,411 - INFO - Epoch 081: Avg. D/G Loss: 4.6234e-02, 8.3117e+01 Avg. reconst. Loss: 1.7541e-01 Remaining Time: 11h 55min 18s LR: 4.61078e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:04:15,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.022 81.178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:04:15,547 - INFO - Epoch 083: Avg. D/G Loss: 5.0934e-02, 7.9189e+01 Avg. reconst. Loss: 1.6901e-01 Remaining Time: 11h 49min 15s LR: 4.60156e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:07:36,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.060 77.064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:07:37,168 - INFO - Epoch 084: Avg. D/G Loss: 1.1035e-01, 8.1418e+01 Avg. reconst. Loss: 1.7039e-01 Remaining Time: 11h 46min 13s LR: 4.59696e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:10:58,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.028 81.381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:10:58,757 - INFO - Epoch 085: Avg. D/G Loss: 5.3741e-02, 8.0989e+01 Avg. reconst. Loss: 1.6824e-01 Remaining Time: 11h 43min 10s LR: 4.59237e-05 \n",
      "2025-06-18 13:10:59,718 - LIGHT_DEBUG - Models/HiFiGAN_v2_epoch_080.pth deleted\n",
      "2025-06-18 13:10:59,719 - LIGHT_DEBUG - Checkpoint saved model to Models/HiFiGAN_v2_epoch_085.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:14:20,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.043 77.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:14:21,272 - INFO - Epoch 086: Avg. D/G Loss: 7.5378e-02, 8.1279e+01 Avg. reconst. Loss: 1.7060e-01 Remaining Time: 11h 40min 07s LR: 4.58777e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:17:42,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.037 74.488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:17:42,844 - INFO - Epoch 087: Avg. D/G Loss: 4.0375e-02, 8.1491e+01 Avg. reconst. Loss: 1.7212e-01 Remaining Time: 11h 37min 04s LR: 4.58319e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:21:04,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.034 80.205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:21:04,516 - INFO - Epoch 088: Avg. D/G Loss: 4.2114e-02, 8.1762e+01 Avg. reconst. Loss: 1.6817e-01 Remaining Time: 11h 34min 01s LR: 4.57860e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:24:25,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.070 81.623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:24:26,228 - INFO - Epoch 089: Avg. D/G Loss: 1.1859e-01, 8.2332e+01 Avg. reconst. Loss: 1.7155e-01 Remaining Time: 11h 30min 57s LR: 4.57402e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:27:47,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.108 81.798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:27:48,302 - INFO - Epoch 090: Avg. D/G Loss: 3.2324e-01, 8.2799e+01 Avg. reconst. Loss: 1.7433e-01 Remaining Time: 11h 27min 53s LR: 4.56945e-05 \n",
      "2025-06-18 13:27:49,271 - LIGHT_DEBUG - Models/HiFiGAN_v2_epoch_085.pth deleted\n",
      "2025-06-18 13:27:49,272 - LIGHT_DEBUG - Checkpoint saved model to Models/HiFiGAN_v2_epoch_090.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:31:11,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.029 82.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:31:11,842 - INFO - Epoch 091: Avg. D/G Loss: 6.9885e-02, 8.2958e+01 Avg. reconst. Loss: 1.7418e-01 Remaining Time: 11h 24min 51s LR: 4.56488e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:34:34,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.063 83.884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:34:34,733 - INFO - Epoch 092: Avg. D/G Loss: 4.5990e-02, 8.3069e+01 Avg. reconst. Loss: 1.7195e-01 Remaining Time: 11h 21min 48s LR: 4.56032e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:37:57,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.027 81.767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:37:57,772 - INFO - Epoch 093: Avg. D/G Loss: 3.8513e-02, 8.3909e+01 Avg. reconst. Loss: 1.7148e-01 Remaining Time: 11h 18min 46s LR: 4.55576e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:41:20,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.083 86.766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:41:20,896 - INFO - Epoch 094: Avg. D/G Loss: 3.7964e-01, 8.3233e+01 Avg. reconst. Loss: 1.8488e-01 Remaining Time: 11h 15min 43s LR: 4.55120e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:44:43,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.198 80.435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:44:43,945 - INFO - Epoch 095: Avg. D/G Loss: 1.0608e-01, 8.1914e+01 Avg. reconst. Loss: 1.9366e-01 Remaining Time: 11h 12min 39s LR: 4.54665e-05 \n",
      "2025-06-18 13:44:44,892 - LIGHT_DEBUG - Models/HiFiGAN_v2_epoch_090.pth deleted\n",
      "2025-06-18 13:44:44,893 - LIGHT_DEBUG - Checkpoint saved model to Models/HiFiGAN_v2_epoch_095.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:48:07,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.041 89.189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:48:08,031 - INFO - Epoch 096: Avg. D/G Loss: 8.7219e-02, 8.3083e+01 Avg. reconst. Loss: 1.8842e-01 Remaining Time: 11h 09min 36s LR: 4.54210e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:51:30,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.029 82.314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:51:31,222 - INFO - Epoch 097: Avg. D/G Loss: 9.3079e-02, 8.4134e+01 Avg. reconst. Loss: 1.7549e-01 Remaining Time: 11h 06min 32s LR: 4.53756e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:54:53,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.087 85.397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:54:54,371 - INFO - Epoch 098: Avg. D/G Loss: 1.2433e-01, 8.5433e+01 Avg. reconst. Loss: 1.8033e-01 Remaining Time: 11h 03min 28s LR: 4.53302e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:58:16,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.028 86.513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:58:17,279 - INFO - Epoch 099: Avg. D/G Loss: 7.1106e-02, 8.4120e+01 Avg. reconst. Loss: 1.8843e-01 Remaining Time: 11h 00min 22s LR: 4.52849e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:01:39,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.071 93.759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:01:40,269 - INFO - Epoch 100: Avg. D/G Loss: 7.3120e-02, 8.5832e+01 Avg. reconst. Loss: 1.7792e-01 Remaining Time: 10h 57min 17s LR: 4.52396e-05 \n",
      "2025-06-18 14:01:41,253 - LIGHT_DEBUG - Models/HiFiGAN_v2_epoch_095.pth deleted\n",
      "2025-06-18 14:01:41,254 - LIGHT_DEBUG - Checkpoint saved model to Models/HiFiGAN_v2_epoch_100.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:05:03,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.072 85.642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:05:04,187 - INFO - Epoch 101: Avg. D/G Loss: 1.6089e-01, 8.7592e+01 Avg. reconst. Loss: 1.7619e-01 Remaining Time: 10h 54min 11s LR: 4.51944e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:08:26,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.036 87.036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:08:27,142 - INFO - Epoch 102: Avg. D/G Loss: 5.2094e-02, 8.5304e+01 Avg. reconst. Loss: 1.7038e-01 Remaining Time: 10h 51min 05s LR: 4.51492e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:11:49,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.047 91.563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:11:50,163 - INFO - Epoch 103: Avg. D/G Loss: 1.6003e-01, 8.7650e+01 Avg. reconst. Loss: 1.7527e-01 Remaining Time: 10h 47min 59s LR: 4.51040e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:15:12,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.046 86.954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:15:13,110 - INFO - Epoch 104: Avg. D/G Loss: 5.5695e-02, 8.4833e+01 Avg. reconst. Loss: 1.6900e-01 Remaining Time: 10h 44min 52s LR: 4.50589e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:18:35,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.038 90.824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:18:36,057 - INFO - Epoch 105: Avg. D/G Loss: 6.9763e-02, 8.6386e+01 Avg. reconst. Loss: 1.7020e-01 Remaining Time: 10h 41min 45s LR: 4.50139e-05 \n",
      "2025-06-18 14:18:36,995 - LIGHT_DEBUG - Models/HiFiGAN_v2_epoch_100.pth deleted\n",
      "2025-06-18 14:18:36,996 - LIGHT_DEBUG - Checkpoint saved model to Models/HiFiGAN_v2_epoch_105.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:21:59,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.573 88.6357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:21:59,869 - INFO - Epoch 106: Avg. D/G Loss: 1.8103e-01, 8.7562e+01 Avg. reconst. Loss: 1.7295e-01 Remaining Time: 10h 38min 37s LR: 4.49688e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:25:22,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.040 84.239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:25:22,830 - INFO - Epoch 107: Avg. D/G Loss: 9.5276e-02, 8.6670e+01 Avg. reconst. Loss: 1.7050e-01 Remaining Time: 10h 35min 29s LR: 4.49239e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:28:45,000 - LIGHT_DEBUG - Batch 078/078 D/G Loss: 0.056 86.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:28:45,785 - INFO - Epoch 108: Avg. D/G Loss: 4.5227e-02, 8.7816e+01 Avg. reconst. Loss: 1.7028e-01 Remaining Time: 10h 32min 21s LR: 4.48790e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 14:30:06,000 - LIGHT_DEBUG - Batch 031/078 D/G Loss: 0.036 93.682"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Training started on {device}\")\n",
    "loss_d_list: list = []\n",
    "loss_g_list: list = []\n",
    "total_time: float = 0.0\n",
    "\n",
    "generator.train()\n",
    "mpd.train()\n",
    "msd.train()\n",
    "for e in range(0, epochs):\n",
    "    total_d_loss: float = 0\n",
    "    total_g_loss: float = 0\n",
    "    total_reconst_loss: float = 0\n",
    "    start_time: float = time.time()\n",
    "\n",
    "    for b_idx, (mel, audio) in enumerate(data_loader):\n",
    "            mel, audio = mel.to(device), audio.to(device).unsqueeze(1)\n",
    "            with torch.autocast(device_type=device):\n",
    "                generated_audio = generator(mel)\n",
    "\n",
    "            generated_mel = T.MelSpectrogram(sample_rate = 32000, n_fft=1023, hop_length=256, n_mels=96, f_min=30).to(device)(generated_audio.squeeze(1))\n",
    "            optim_d.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type=device):\n",
    "                real_mpd_scores, fake_mpd_scores, _, _ = mpd(audio, generated_audio.detach())\n",
    "            loss_d_s, mpd_loss_real, mpd_loss_fake = discriminator_loss(real_mpd_scores, fake_mpd_scores)\n",
    "            \n",
    "            with torch.autocast(device_type=device):\n",
    "                real_msd_scores, fake_msd_scores, _, _ = msd(audio, generated_audio.detach())\n",
    "            loss_d_f, msd_loss_real, msd_loss_fake = discriminator_loss(real_msd_scores, fake_msd_scores)\n",
    "\n",
    "            disc_loss: Tensor = loss_d_s + loss_d_f\n",
    "            total_d_loss += disc_loss\n",
    "\n",
    "            disc_loss.backward()\n",
    "            optim_d.step()\n",
    "\n",
    "            optim_g.zero_grad()\n",
    "\n",
    "            mel_loss = F.l1_loss(mel, generated_mel) * 45\n",
    "\n",
    "            with torch.autocast(device_type=device):\n",
    "                real_mpd_scores, fake_mpd_scores, real_mpd_features, fake_mpd_features = mpd(audio, generated_audio)\n",
    "                real_msd_scores, fake_msd_scores, real_msd_features, fake_msd_features = msd(audio, generated_audio)\n",
    "            \n",
    "            mpd_feature_loss = feature_loss(real_mpd_features, fake_mpd_features)\n",
    "            msd_feature_loss = feature_loss(real_msd_features, fake_msd_features)\n",
    "            mpd_gen_loss, _ = generator_loss(fake_mpd_scores)\n",
    "            msd_gen_loss, _ = generator_loss(fake_msd_scores)\n",
    "            \n",
    "            total_reconst_loss += F.l1_loss(audio, generated_audio)\n",
    "\n",
    "            gen_loss = mpd_gen_loss + msd_gen_loss + mpd_feature_loss + msd_feature_loss + mel_loss\n",
    "            total_g_loss += gen_loss\n",
    "\n",
    "            gen_loss.backward()\n",
    "            optim_g.step()\n",
    "\n",
    "            if logger.getEffectiveLevel() == LIGHT_DEBUG:\n",
    "                current_batch = b_idx + 1\n",
    "                print(f\"\\r{time.strftime('%Y-%m-%d %H:%M:%S')},000 - LIGHT_DEBUG - Batch {current_batch:03d}/{len(data_loader):03d} D/G Loss: {disc_loss.item():.3f} {gen_loss.item():.3f}\", end='', flush=True)\n",
    "    else:\n",
    "        if logger.getEffectiveLevel() == LIGHT_DEBUG:\n",
    "            print(flush=True)\n",
    "\n",
    "        avg_d_loss = total_d_loss / len(data_loader)\n",
    "        avg_g_loss = total_g_loss / len(data_loader)\n",
    "        avg_reconst_loss = total_reconst_loss / len(data_loader)\n",
    "        loss_d_list.append(avg_d_loss)\n",
    "        loss_g_list.append(avg_g_loss)\n",
    "        if gen_lr_scheduler is not None:\n",
    "            gen_lr_scheduler.step()\n",
    "        if disc_lr_scheduler is not None:\n",
    "            disc_lr_scheduler.step()\n",
    "\n",
    "        sw.add_scalar(\"training/gen_loss\", avg_g_loss, e)\n",
    "        sw.add_scalar(\"training/disc_loss\", avg_d_loss, e)\n",
    "        sw.add_scalar(\"training/reconstr_loss\", avg_reconst_loss, e)\n",
    "        sw.add_scalar(\"training/lr\", optim_g.param_groups[0][\"lr\"], e)\n",
    "        sw.flush()\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        total_time += epoch_time\n",
    "        remaining_time = int((total_time / (e + 1)) * (epochs - e - 1))\n",
    "\n",
    "        logger.info(f\"Epoch {e + 1:03d}: Avg. D/G Loss: {avg_d_loss:.4e}, {avg_g_loss:.4e} Avg. reconst. Loss: {avg_reconst_loss:.4e} Remaining Time: {remaining_time // 3600:02d}h {(remaining_time % 3600) // 60:02d}min {round(remaining_time % 60):02d}s LR: {optim_d.param_groups[0]['lr']:.5e} \")\n",
    "        \n",
    "        if checkpoint_freq > 0 and (e + 1) % checkpoint_freq == 0:\n",
    "            checkpoint_path: str = f\"{full_model_path[:-4]}_epoch_{e + 1:03d}.pth\"\n",
    "            torch.save({\"generator\": generator.state_dict(), \"msd\": msd.state_dict(), \"mpd\": mpd.state_dict(), \"optim_g\": optim_g.state_dict(), \"optim_d\": optim_d.state_dict() , \"epoch\": e + 1}, checkpoint_path)\n",
    "            if e + 1 != checkpoint_freq:\n",
    "                last_path: str = f\"{full_model_path[:-4]}_epoch_{(e + 1) - checkpoint_freq:03d}.pth\"\n",
    "                del_if_exists(last_path)\n",
    "            logger.light_debug(f\"Checkpoint saved model to {checkpoint_path}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "torch.save({\"generator\": generator.state_dict(), \"msd\": msd.state_dict(), \"mpd\": mpd.state_dict(), \"optim_g\": optim_g.state_dict(), \"optim_d\": optim_d.state_dict() , \"epoch\": e + 1}, full_model_path)\n",
    "\n",
    "logger.light_debug(f\"Saved model to {full_model_path}\")\n",
    "\n",
    "if checkpoint_freq > 0:\n",
    "    checkpoint_path: str = f\"{full_model_path[:-4]}_epoch_{e + 1 - ((e + 1) % checkpoint_freq):03d}.pth\"\n",
    "    del_if_exists(checkpoint_path)\n",
    "\n",
    "scatter_plot(loss_d_list)\n",
    "scatter_plot(loss_g_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1267dffe",
   "metadata": {},
   "source": [
    "#### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8620c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_model() -> nn.Module:\n",
    "    generator = Generator(\n",
    "                    n_mel_channels=96,\n",
    "                    upsample_rates=[8,8,2,2],\n",
    "                    upsample_kernel_sizes=[16,16,4,4],\n",
    "                    upsample_initial_channel=512,\n",
    "                    resblock_kernel_sizes=[3,7,11],\n",
    "                    resblock_dilation_sizes=[[1,3,5], [1,3,5], [1,3,5]],\n",
    "                    resblock=1\n",
    "                ).to(device)\n",
    "    mpd = MultiPeriodDiscriminator().to(device)\n",
    "    msd = MultiScaleDiscriminator().to(device)\n",
    "    return generator, mpd, msd\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    b1 = trial.suggest_float(\"b1\", 0.4, 0.99)\n",
    "    b2 = trial.suggest_float(\"b2\", 0.4, 0.999)\n",
    "    lr_decay = trial.suggest_float(\"lr_decay\", 0.7, 0.99999)\n",
    "    generator, mpd, msd = static_model()\n",
    "    optim_g = torch.optim.AdamW(generator.parameters(), lr, betas=[b1, b2])\n",
    "    optim_d = torch.optim.AdamW(itertools.chain(msd.parameters(), mpd.parameters()),\n",
    "                                    lr, betas=[b1, b2])\n",
    "    gen_lr_scheduler = optim.lr_scheduler.ExponentialLR(optim_g, gamma=lr_decay)\n",
    "    disc_lr_scheduler = optim.lr_scheduler.ExponentialLR(optim_d, gamma=lr_decay)\n",
    "\n",
    "    \n",
    "    n_epochs = 10\n",
    "    best_reconst_loss: float = float('inf')\n",
    "    generator.train()\n",
    "    mpd.train()\n",
    "    msd.train()\n",
    "    for e in range(0, n_epochs):\n",
    "        total_reconst_loss: float = 0\n",
    "\n",
    "        for b_idx, (mel, audio) in enumerate(data_loader):\n",
    "            mel, audio = mel.to(device), audio.to(device).unsqueeze(1)\n",
    "            with torch.autocast(device_type=device):\n",
    "                generated_audio = generator(mel)\n",
    "\n",
    "            generated_mel = T.MelSpectrogram(sample_rate = 32000, n_fft=1023, hop_length=256, n_mels=96, f_min=30).to(device)(generated_audio.squeeze(1))\n",
    "            optim_d.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type=device):\n",
    "                real_mpd_scores, fake_mpd_scores, _, _ = mpd(audio, generated_audio.detach())\n",
    "            loss_d_s, mpd_loss_real, mpd_loss_fake = discriminator_loss(real_mpd_scores, fake_mpd_scores)\n",
    "            \n",
    "            with torch.autocast(device_type=device):\n",
    "                real_msd_scores, fake_msd_scores, _, _ = msd(audio, generated_audio.detach())\n",
    "            loss_d_f, msd_loss_real, msd_loss_fake = discriminator_loss(real_msd_scores, fake_msd_scores)\n",
    "\n",
    "            total_disc_loss: Tensor = loss_d_s + loss_d_f\n",
    "\n",
    "            total_disc_loss.backward()\n",
    "            optim_d.step()\n",
    "\n",
    "            optim_g.zero_grad()\n",
    "\n",
    "            mel_loss = F.l1_loss(mel, generated_mel) * 45\n",
    "\n",
    "            with torch.autocast(device_type=device):\n",
    "                real_mpd_scores, fake_mpd_scores, real_mpd_features, fake_mpd_features = mpd(audio, generated_audio)\n",
    "                real_msd_scores, fake_msd_scores, real_msd_features, fake_msd_features = msd(audio, generated_audio)\n",
    "            \n",
    "            mpd_feature_loss = feature_loss(real_mpd_features, fake_mpd_features)\n",
    "            msd_feature_loss = feature_loss(real_msd_features, fake_msd_features)\n",
    "            mpd_gen_loss, _ = generator_loss(fake_mpd_scores)\n",
    "            msd_gen_loss, _ = generator_loss(fake_msd_scores)\n",
    "            \n",
    "            total_reconst_loss += (F.l1_loss(audio, generated_audio) + mel_loss / 45) / 2\n",
    "\n",
    "            total_gen_loss = mpd_gen_loss + msd_gen_loss + mpd_feature_loss + msd_feature_loss + mel_loss\n",
    "\n",
    "            total_gen_loss.backward()\n",
    "            optim_g.step()\n",
    "\n",
    "            avg_reconst_loss = total_reconst_loss / len(data_loader)\n",
    "\n",
    "\n",
    "            gen_lr_scheduler.step()\n",
    "            disc_lr_scheduler.step()\n",
    "\n",
    "    \n",
    "        if avg_reconst_loss < best_reconst_loss:\n",
    "            best_reconst_loss = avg_reconst_loss\n",
    "        trial.report(avg_reconst_loss, e)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    return best_reconst_loss\n",
    "\n",
    "def run_optim(n_trials: int, name: str =\"main_study_wave\") -> None:\n",
    "    study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(),study_name=name)\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    logger.info(\"Finished Study\")\n",
    "    logger.info(f\"Best trial: {study.best_trial} with value: {study.best_trial.value} using params:\")\n",
    "    for key, val in study.best_trial.params.items():\n",
    "        logger.info(f\"{key}:{val}\")\n",
    "    \n",
    "    logger.info(\"Param importance:\")\n",
    "    for param, importance in get_param_importances(study).items():\n",
    "            logger.info(f\"{param}: {importance:.4f}\")\n",
    "    fig = plot_param_importances(study)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b81c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 17:57:06,176] A new study created in memory with name: main_study\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "[I 2025-06-16 18:19:40,773] Trial 0 finished with value: 0.23749111592769623 and parameters: {'lr': 0.0006364028564452524, 'b1': 0.7872070788505228, 'b2': 0.5387522714858821, 'lr_decay': 0.8797010728452869}. Best is trial 0 with value: 0.23749111592769623.\n",
      "[I 2025-06-16 18:42:15,557] Trial 1 finished with value: 0.2389201670885086 and parameters: {'lr': 0.0006317571936581328, 'b1': 0.9317838884335691, 'b2': 0.6985585219527081, 'lr_decay': 0.9189079868516059}. Best is trial 0 with value: 0.23749111592769623.\n",
      "[I 2025-06-16 19:04:53,468] Trial 2 finished with value: 0.23809592425823212 and parameters: {'lr': 0.00015222819211399175, 'b1': 0.8835259616332495, 'b2': 0.6759253199716412, 'lr_decay': 0.7871656983646544}. Best is trial 0 with value: 0.23749111592769623.\n",
      "[I 2025-06-16 19:27:31,760] Trial 3 finished with value: 0.23827217519283295 and parameters: {'lr': 0.00035883244083511793, 'b1': 0.6676474017690415, 'b2': 0.8710038483127349, 'lr_decay': 0.7170858922185632}. Best is trial 0 with value: 0.23749111592769623.\n",
      "[I 2025-06-16 19:50:10,517] Trial 4 finished with value: 0.23372061550617218 and parameters: {'lr': 0.0006747806202725336, 'b1': 0.5399390253848708, 'b2': 0.4856804377129979, 'lr_decay': 0.895343440571774}. Best is trial 4 with value: 0.23372061550617218.\n",
      "[I 2025-06-16 19:52:27,240] Trial 5 pruned. \n",
      "[I 2025-06-16 19:54:44,032] Trial 6 pruned. \n",
      "[I 2025-06-16 20:17:22,456] Trial 7 finished with value: 0.23650112748146057 and parameters: {'lr': 2.1043083377195645e-05, 'b1': 0.5296853826742518, 'b2': 0.9538869063659376, 'lr_decay': 0.9240631448231281}. Best is trial 4 with value: 0.23372061550617218.\n",
      "[I 2025-06-16 20:19:38,736] Trial 8 pruned. \n",
      "[I 2025-06-16 20:21:55,033] Trial 9 pruned. \n",
      "[I 2025-06-16 20:24:11,931] Trial 10 pruned. \n",
      "[I 2025-06-16 20:26:28,426] Trial 11 pruned. \n",
      "[I 2025-06-16 20:28:44,627] Trial 12 pruned. \n",
      "[I 2025-06-16 20:31:01,645] Trial 13 pruned. \n",
      "[I 2025-06-16 20:53:40,574] Trial 14 finished with value: 0.23773851990699768 and parameters: {'lr': 3.2918510107403263e-05, 'b1': 0.5894135191422346, 'b2': 0.5849460757631223, 'lr_decay': 0.8559154021750328}. Best is trial 4 with value: 0.23372061550617218.\n",
      "[I 2025-06-16 21:16:21,357] Trial 15 finished with value: 0.2290896624326706 and parameters: {'lr': 4.972596409463262e-05, 'b1': 0.49535133368856016, 'b2': 0.8952694447960506, 'lr_decay': 0.9539652906543938}. Best is trial 15 with value: 0.2290896624326706.\n",
      "[I 2025-06-16 21:39:01,080] Trial 16 finished with value: 0.2347373217344284 and parameters: {'lr': 4.9769795983242464e-05, 'b1': 0.6212483439723553, 'b2': 0.8582768719917313, 'lr_decay': 0.9952255174022536}. Best is trial 15 with value: 0.2290896624326706.\n",
      "[I 2025-06-16 21:41:17,740] Trial 17 pruned. \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://n0vefb5fz4.clg07azjl.paperspacegradient.com/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "run_optim(25, \"main_study\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50424b99",
   "metadata": {},
   "source": [
    "### Convert to wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32a8a83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:06:27,604 - LIGHT_DEBUG - Normalized to range: [-0.99999,0.99999]\n",
      "2025-06-18 15:06:27,614 - LIGHT_DEBUG - Saved file to:HiFiGAN_v2_0.wav\n",
      "2025-06-18 15:06:27,615 - LIGHT_DEBUG - Normalized to range: [-0.99999,0.99999]\n",
      "2025-06-18 15:06:27,624 - LIGHT_DEBUG - Saved file to:HiFiGAN_real_v2.wav\n"
     ]
    }
   ],
   "source": [
    "file_idx: int = 4000\n",
    "with torch.no_grad():\n",
    "    generated_wave = generator(torch.tensor(mel_data[file_idx]).unsqueeze(0).to(device))\n",
    "save_audio_file(generated_wave.cpu().numpy()[0,0], \"HiFiGAN_v2_0.wav\", 32000)\n",
    "#save_audio_file(librosa.feature.inverse.mel_to_audio(mel_data[file_idx], n_fft=1023, hop_length=256, sr=32000), \"test_gl.wav\", 32000)\n",
    "save_audio_file(audio_data[file_idx], \"HiFiGAN_real_v2.wav\", 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b224011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:08:36,719 - LIGHT_DEBUG - spectrogram loaded from Results/spect_1.npz of shape: (96, 512)\n",
      "2025-06-18 15:08:36,763 - LIGHT_DEBUG - Normalized to range: [-0.99999,0.99999]\n",
      "2025-06-18 15:08:36,777 - LIGHT_DEBUG - Saved file to:HiFiGAN_v2_muGen_1.wav\n"
     ]
    }
   ],
   "source": [
    "spect = load_spectrogram(\"Results/spect_1.npz\")\n",
    "with torch.no_grad():\n",
    "    generated_wave = generator(torch.tensor(spect).unsqueeze(0).to(device))\n",
    "save_audio_file(generated_wave.cpu().numpy()[0,0], \"HiFiGAN_v2_muGen_1.wav\", 32000)\n",
    "#save_audio_file(librosa.feature.inverse.mel_to_audio(spect, n_fft=1023, hop_length=256, sr=32000), \"muGen_out1_gl.wav\", 32000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

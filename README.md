# MA_DDPM

| Date       | Content              | Problems             |
| :---       | :---                 | :---                 |
| >21.02.2025 | Create a General Structure, with a Preprocessing, Train, Eval and Util files. Important Data Can be set in the Conf.py file. The processing creats n second splits and the converts those to STFT spectograms.  | Using Linear Layers in the VAE will lead to too many Parameters when the VAE does not have a high compression rate. Therefore I implementet a convolutional bottleneck which drastically lowers parameter counts. |
|>23.02.2025 | It seems that even though the results of the VAE got better, due too the Convolutional Architecture the Bottleneck has a very high dimensionality due to larg number of filters. And further Compression is not an option so due to the small size of my spectograms Latent Diffusion seems to be impractical. Training in Paperspace of conv_VAE_v2 with batch size lr = 1-e4, batch = 32. >1000 epochs on dataset 640 and 1280, weight of reprod loss = 10'000. Reprod loss (*weight) = 1.9e5, KL = 4e4 | VAE does not seem to be the way to go, it induces too much noise and the error does not seem to get lower with lots of epochs. |
| >03.03.2025 | I implemented the Diffusion and the corresponding UNET the first try was very unsuccesfull(Totally noisy output no characteristics whatsoever). Looking closer at the data it seems that UNET was implemented incorrectly with scaling the noise to 0-1 as last layer (Sigmoid) which prevented it from precisely predicting noise, furthermore i had to switch the training data interval to [-1 1] rather than [0, 1] as the noise added is of form N(0, I). Furthermore i added gradient scaling and mixed precision training with cuda and also the possibility to use gradient accumulation, as i can only train with batch size 8 due to the model having ~450k params.| Optimize the model further to make it more memory efficient or upgrade and use bigger GPU's. |
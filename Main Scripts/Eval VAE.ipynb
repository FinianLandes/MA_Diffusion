{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval VAE\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Dir \n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "# Utils\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import logging\n",
    "# Base Scripts\n",
    "from Libraries.VAE import *\n",
    "from Libraries.Utils import *\n",
    "from Conf import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name: str = \"conv_VAE_v2\"\n",
    "training_data_name: str = \"training_640\"\n",
    "logging_level: int = logging.INFO\n",
    "logging.basicConfig(level=logging_level, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger: logging.Logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(model: nn.Module, device: str,  n_samples: int = 1, seed: ndarray = None, file_name: str = \"test\") -> None:\n",
    "    model.eval()\n",
    "    if seed is not None:\n",
    "        save_audio_file(spectrogram_to_audio(unnormalize(seed), LEN_FFT), f\"{RESULT_PATH}/{file_name}_seed.wav\")\n",
    "        samples = generate_sample(model, device, Tensor(seed).view(1, 1, seed.shape[-2], seed.shape[-1]), num_samples=n_samples)\n",
    "    else:\n",
    "        samples = generate_sample(model, device, num_samples=n_samples)\n",
    "    for i in range(samples.shape[1]):\n",
    "        save_audio_file(spectrogram_to_audio(unnormalize(samples[0,i]), LEN_FFT), f\"{RESULT_PATH}/{file_name}_{i:02d}.wav\", SAMPLE_RATE)\n",
    "    logger.light_debug(f\"Saved {samples.shape[0]} samples to {RESULT_PATH}\")\n",
    "\n",
    "\n",
    "def pass_through(model: nn.Module, device: str, sample: ndarray, file_name: str = \"test\") -> None:\n",
    "    visualize_spectogram(sample)\n",
    "    save_audio_file(spectrogram_to_audio(unnormalize(sample), LEN_FFT), f\"{RESULT_PATH}/{file_name}_inp.wav\")\n",
    "    x = fwd_pass(model, device, Tensor(sample).view(1, 1, sample.shape[-2], sample.shape[-1]))\n",
    "    visualize_spectogram(x[0,0])\n",
    "    audio = spectrogram_to_audio(unnormalize(x[0,0]), LEN_FFT)\n",
    "    save_audio_file(audio, f\"{RESULT_PATH}/{file_name}_out.wav\")\n",
    "    logger.light_debug(f\"Saved passed through sample to {RESULT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading & Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 21:02:17,629 - INFO - Data loaded with shape: (640, 1024, 672)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = load_training_data(f\"{DATA_PATH}/{training_data_name}.npy\")\n",
    "logger.info(f\"Data loaded with shape: {file.shape}\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = VAE(in_channels=1, latent_dim=256, device=device,input_shape=[0,0, file.shape[-2], file.shape[-1]], n_conv_blocks=1, n_starting_filters=64, lin_bottleneck=False).to(device)\n",
    "model.load_state_dict(torch.load(f\"{MODEL_PATH}/{model_name}.pth\", map_location=device, weights_only=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data(model=model,device=device, seed=file[600])\n",
    "#pass_through(model, device, file[300])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

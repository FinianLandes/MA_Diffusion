{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Diffusion Model\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Dir \n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "# Utils\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from torchsummary import summary\n",
    "import logging\n",
    "\n",
    "# Base Scripts\n",
    "from Libraries.U_Net import *\n",
    "from Libraries.Diffusion import *\n",
    "from Libraries.Utils import *\n",
    "from Conf import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config\n",
    "General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_level: int = LIGHT_DEBUG #logging.INFO\n",
    "model_name: str = \"diffusion_v1\"\n",
    "model_path: str = f\"{MODEL_PATH}/{model_name}.pth\"\n",
    "checkpoint_freq: int = 5 #0 for no checkpoint saving\n",
    "training_data_name: str = \"training_1280\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "batch_size: int = 2\n",
    "epochs: int = 100\n",
    "diffusion_timesteps: int = 500\n",
    "\n",
    "logging.basicConfig(level=logging_level, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger: logging.Logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate: float = 1e-5\n",
    "lr_decay: int = 40\n",
    "lr_gamma: float = 0.1\n",
    "n_starting_filters: int = 24\n",
    "n_blocks: int = 2 #Each samples down by factor of 2\n",
    "n_groups: int = 8 #For group norm\n",
    "time_embed_dim: int = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 22:50:12,950 - LIGHT_DEBUG - Ndarray loaded from ../Data/training_1280.npy of shape: (1280, 1024, 672)\n"
     ]
    }
   ],
   "source": [
    "file = load_training_data(f\"{DATA_PATH}/{training_data_name}.npy\")[:4, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 22:50:12,971 - INFO - Data loaded with shape: (4, 1024, 672)\n"
     ]
    }
   ],
   "source": [
    "data_loader = create_dataloader(Audio_Data(file), batch_size)\n",
    "logger.info(f\"Data loaded with shape: {file.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation\n",
    "U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 22:50:12,999 - INFO - Model diffusion_v1 created with 452065 Parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      " TimestepEmbedding-1                   [-1, 64]               0\n",
      "            Conv2d-2        [-1, 24, 1024, 672]             240\n",
      "         GroupNorm-3        [-1, 24, 1024, 672]              48\n",
      "         LeakyReLU-4        [-1, 24, 1024, 672]               0\n",
      "         LeakyReLU-5        [-1, 24, 1024, 672]               0\n",
      "         LeakyReLU-6        [-1, 24, 1024, 672]               0\n",
      "         LeakyReLU-7        [-1, 24, 1024, 672]               0\n",
      "         LeakyReLU-8        [-1, 24, 1024, 672]               0\n",
      "            Conv2d-9        [-1, 48, 1024, 672]          10,416\n",
      "        LeakyReLU-10        [-1, 48, 1024, 672]               0\n",
      "        LeakyReLU-11        [-1, 48, 1024, 672]               0\n",
      "        LeakyReLU-12        [-1, 48, 1024, 672]               0\n",
      "        LeakyReLU-13        [-1, 48, 1024, 672]               0\n",
      "        LeakyReLU-14        [-1, 48, 1024, 672]               0\n",
      "           Conv2d-15            [-1, 192, 1, 1]          12,480\n",
      "        MaxPool2d-16         [-1, 48, 512, 336]               0\n",
      "           Conv2d-17         [-1, 96, 512, 336]          41,568\n",
      "        GroupNorm-18         [-1, 96, 512, 336]             192\n",
      "        LeakyReLU-19         [-1, 96, 512, 336]               0\n",
      "        LeakyReLU-20         [-1, 96, 512, 336]               0\n",
      "        LeakyReLU-21         [-1, 96, 512, 336]               0\n",
      "        LeakyReLU-22         [-1, 96, 512, 336]               0\n",
      "        LeakyReLU-23         [-1, 96, 512, 336]               0\n",
      "           Conv2d-24        [-1, 192, 512, 336]         166,080\n",
      "        LeakyReLU-25        [-1, 192, 512, 336]               0\n",
      "        LeakyReLU-26        [-1, 192, 512, 336]               0\n",
      "        LeakyReLU-27        [-1, 192, 512, 336]               0\n",
      "        LeakyReLU-28        [-1, 192, 512, 336]               0\n",
      "        LeakyReLU-29        [-1, 192, 512, 336]               0\n",
      "    ConvBlockDown-30        [-1, 192, 512, 336]               0\n",
      "           Conv2d-31             [-1, 48, 1, 1]           3,120\n",
      "         Upsample-32       [-1, 192, 1024, 672]               0\n",
      "  ConvTranspose2d-33        [-1, 96, 1024, 672]         165,984\n",
      "        LeakyReLU-34        [-1, 96, 1024, 672]               0\n",
      "        LeakyReLU-35        [-1, 96, 1024, 672]               0\n",
      "        LeakyReLU-36        [-1, 96, 1024, 672]               0\n",
      "        LeakyReLU-37        [-1, 96, 1024, 672]               0\n",
      "        LeakyReLU-38        [-1, 96, 1024, 672]               0\n",
      "  ConvTranspose2d-39        [-1, 48, 1024, 672]          41,520\n",
      "        LeakyReLU-40        [-1, 48, 1024, 672]               0\n",
      "        LeakyReLU-41        [-1, 48, 1024, 672]               0\n",
      "        LeakyReLU-42        [-1, 48, 1024, 672]               0\n",
      "        LeakyReLU-43        [-1, 48, 1024, 672]               0\n",
      "        LeakyReLU-44        [-1, 48, 1024, 672]               0\n",
      "      ConvBlockUp-45        [-1, 48, 1024, 672]               0\n",
      "  ConvTranspose2d-46        [-1, 24, 1024, 672]          10,392\n",
      "        LeakyReLU-47        [-1, 24, 1024, 672]               0\n",
      "        LeakyReLU-48        [-1, 24, 1024, 672]               0\n",
      "        LeakyReLU-49        [-1, 24, 1024, 672]               0\n",
      "        LeakyReLU-50        [-1, 24, 1024, 672]               0\n",
      "        LeakyReLU-51        [-1, 24, 1024, 672]               0\n",
      "           Conv2d-52         [-1, 1, 1024, 672]              25\n",
      "          Sigmoid-53         [-1, 1, 1024, 672]               0\n",
      "            U_NET-54         [-1, 1, 1024, 672]               0\n",
      "================================================================\n",
      "Total params: 452,065\n",
      "Trainable params: 452,065\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.62\n",
      "Forward/backward pass size (MB): 11670.75\n",
      "Params size (MB): 1.72\n",
      "Estimated Total Size (MB): 11675.10\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "u_net = U_NET(in_channels=1, device=device, input_shape=[0, 0, file.shape[-2], file.shape[-1]], n_res_layers=n_blocks, n_starting_filters=n_starting_filters, n_groups=n_groups, time_emb_dim=time_embed_dim).to(device)\n",
    "if os.path.exists(model_path):\n",
    "    u_net.load_state_dict(torch.load(model_path, weights_only=False, map_location=device))\n",
    "    logger.info(f\"Model {model_name} loaded with {count_parameters(u_net)} Parameters\")\n",
    "else: \n",
    "    logger.info(f\"Model {model_name} created with {count_parameters(u_net)} Parameters\")\n",
    "\n",
    "optimizer = optim.AdamW(u_net.parameters(), lr=learning_rate, weight_decay=0.05)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay, gamma=lr_gamma)\n",
    "u_net = nn.DataParallel(u_net)\n",
    "u_net = u_net.to(device)\n",
    "#summary(u_net, [(1, 1024, 672), (1, 1, 1)])# ######akscOICCXSASSSassddff#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = Diffusion(u_net=u_net, u_net_optimizer=optimizer,diffusion_timesteps=diffusion_timesteps)\n",
    "noise_schedule = cosine_noise(T=diffusion_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 22:51:55,000 - LIGHT_DEBUG - Batch 02/02"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 22:51:55,897 - INFO - Epoch 01: Avg. Loss: 6.07030e-01 Remaining Time: 02h 16min 34s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-03-02 22:53:26,000 - LIGHT_DEBUG - Batch 02/02"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 22:53:26,020 - INFO - Epoch 02: Avg. Loss: 6.12806e-01 Remaining Time: 02h 21min 11s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-03-02 22:53:56,000 - LIGHT_DEBUG - Batch 01/02"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mU_Net_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_schedule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_schedule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\finia\\OneDrive - SBL\\MA\\MA_DDPM\\Libraries\\Diffusion.py:60\u001b[0m, in \u001b[0;36mDiffusion.train\u001b[1;34m(self, data_loader, device, epochs, loss_function, noise_schedule, checkpoint_freq, model_path)\u001b[0m\n\u001b[0;32m     56\u001b[0m     loss: Tensor \u001b[38;5;241m=\u001b[39m loss_function(pred_noise, eps)\n\u001b[0;32m     58\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 60\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     62\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[1;32mc:\\Users\\finia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\finia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\finia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "diffusion.train(data_loader, device, epochs=epochs, loss_function=U_Net_loss, noise_schedule=noise_schedule, checkpoint_freq=checkpoint_freq, model_path=model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

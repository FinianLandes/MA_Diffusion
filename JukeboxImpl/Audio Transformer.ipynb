{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c14a49aa",
   "metadata": {},
   "source": [
    "# Transformer for Generation of music using VQVAE outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e0b0f1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be45533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    import librosa\n",
    "except:\n",
    "    !pip install librosa\n",
    "try: \n",
    "    import optuna, plotly\n",
    "except:\n",
    "    !pip install optuna\n",
    "    !pip install plotly\n",
    "\n",
    "\n",
    "#Set Dir \n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import optuna, plotly\n",
    "from optuna.importance import get_param_importances\n",
    "from optuna.visualization import plot_param_importances\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from collections import Counter\n",
    "# Utils\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import logging, math\n",
    "\n",
    "\n",
    "# Base Scripts\n",
    "from Libraries.Utils import *\n",
    "from Libraries.VQ_VAE import *\n",
    "from Libraries.Transformer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470240a6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75d25f2",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8dd323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_kernel: bool = True\n",
    "device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "training_data_name: str = \"../Data/training_v2_full\"\n",
    "test_data_name: str = \"../Data/unseen_test_data.npy\"\n",
    "vq_vae_name: str = \"VQ_VAE_v2\"\n",
    "vq_vae_path: str = OS().path_to_remote_path(\"../Models/{}\".format(vq_vae_name), remote_kernel)\n",
    "model_name: str = \"transformer_v2_2\"\n",
    "full_model_path: str = OS().path_to_remote_path(\"../Models/{}.pth\".format(model_name), remote_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a3f910",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eda145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_level: int = logging.INFO\n",
    "logging.basicConfig(level=logging_level, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger: logging.Logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f964eab",
   "metadata": {},
   "source": [
    "### Data Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b642b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size: int = 16\n",
    "n_workers: int = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274c5b37",
   "metadata": {},
   "source": [
    "### Load VQ VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8b78a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_vae = VQVAE(input_emb_width=1, output_emb_width=64, k_bins=2048, levels=1, downs_t=[3], strides_t=[2]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59360fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(vq_vae_path):\n",
    "    model = torch.load(vq_vae_path, map_location=device)\n",
    "    vq_vae.load_state_dict(model[\"vq_vae\"])\n",
    "    logger.info(f\"Model {vq_vae_name} loaded with {count_parameters(vq_vae)} Parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54d106b",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b7e91ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ModelData()\n",
    "md.load_data_from_path(data_path=OS().path_to_remote_path(training_data_name, remote_kernel))\n",
    "md.create_validation_split()\n",
    "train_dataset, val_dataset  = md.create_datasets()\n",
    "logger.info(f\"Created train dataset with length {len(md.train_dataset)} and validation dataset with length {len(md.val_dataset)}\")\n",
    "del md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "859de0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ModelData()\n",
    "md.load_data_from_path(data_path=OS().path_to_remote_path(test_data_name, remote_kernel))\n",
    "md.create_validation_split()\n",
    "test_dataset, _ = md.create_datasets()\n",
    "test_dataloader, _ = md.create_dataloaders(batch_size, num_workers=n_workers)\n",
    "logger.info(f\"Created test dataset with length {len(md.train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c9537cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_repr(vq_vae: VQVAE, data: Dataset, batch_size: int = 24, device: str = \"cpu\", n_samples: int | None = None) -> Dataset:\n",
    "    data_loader = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
    "    converted_data = []\n",
    "    vq_vae.eval()\n",
    "    for b,_ in (data_loader):\n",
    "        with torch.no_grad():\n",
    "            indicies = vq_vae.encode(b.unsqueeze(1).to(device))\n",
    "        converted_data.append(indicies)\n",
    "    vq_vae.train()\n",
    "    new_data = torch.cat(converted_data, dim=0)\n",
    "    if n_samples is not None:\n",
    "        new_data = new_data[:n_samples, ...]\n",
    "    \n",
    "    return AudioDataset(new_data, data_type=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a64d3e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dataset = generate_latent_repr(vq_vae, train_dataset, 24, device)\n",
    "latent_val_dataset = generate_latent_repr(vq_vae, val_dataset, 24, device)\n",
    "train_dataloader = DataLoader(latent_dataset, batch_size, shuffle=False)\n",
    "validation_dataloader = DataLoader(latent_val_dataset, batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8244493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vq_vae.eval()\n",
    "inp = train_dataset[:24][0].unsqueeze(1).to(device)\n",
    "with torch.no_grad():\n",
    "    output, q_z, commit_loss, metrics = vq_vae(inp)\n",
    "    outp = vq_vae.decode(latent_val_dataset[:24][0].to(device))\n",
    "\n",
    "for idx in range(2):\n",
    "    print(q_z[idx])\n",
    "    print(latent_val_dataset[idx][0])\n",
    "    plt.plot(output[idx][0].cpu().numpy())\n",
    "    plt.title(\"Directly decoded\")\n",
    "    plt.show()\n",
    "    plt.plot(outp[idx][0].cpu().numpy())\n",
    "    plt.title(\"Indirectly decoded\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceb848aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_test_dataset = generate_latent_repr(vq_vae, test_dataset, 12, device)\n",
    "latent_test_dataloader = DataLoader(latent_test_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d79ef74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_vae.eval()\n",
    "with torch.no_grad():\n",
    "    outp = vq_vae.decode(latent_test_dataset[:24][0].to(device))\n",
    "\n",
    "plt.plot(outp[4][0].cpu().numpy())\n",
    "plt.title(\"Indirectly decoded\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cc2a137",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = torch.cat([x.flatten() for x, _ in train_dataloader]).tolist()\n",
    "token_counts = Counter(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e872df2",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ce5df6",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50e1ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1, b2 = (0.9, 0.95)\n",
    "lr = 2e-4\n",
    "n_epochs = 100 \n",
    "restart_training: bool = True\n",
    "training_seq_len: int = 2048\n",
    "checkpoint_freq: int = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4c97f1",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f1b1036",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.qkv_proj(x).view(B, T, self.num_heads, 3 * self.head_dim)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if attn_mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(attn_mask == 0, float('-inf'))\n",
    "        attn = F.softmax(attn_scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = (attn @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_mult=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_mult * embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ff_mult * embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.attn(self.norm1(x), attn_mask=mask)\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class GPTDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size=2048, max_seq_len=2048, embed_dim=512, num_heads=8, num_layers=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, max_seq_len, embed_dim))\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GPTBlock(embed_dim, num_heads, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.size()\n",
    "        token_embeddings = self.token_embed(idx)\n",
    "        pos_embeddings = self.pos_embed[:, :T, :]\n",
    "        x = self.drop(token_embeddings + pos_embeddings)\n",
    "\n",
    "        mask = torch.tril(torch.ones(T, T, device=idx.device)).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask=mask)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13fdfd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = GPTDecoder().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa22fbc",
   "metadata": {},
   "source": [
    "### Optimizers & Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd975db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(transformer.parameters(), lr, (b1, b2), weight_decay=0.1)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=10000, num_training_steps=n_epochs * len(train_dataloader), last_epoch=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23bcf4b",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c340570",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(full_model_path):\n",
    "    model = torch.load(full_model_path, map_location=device)\n",
    "    transformer.load_state_dict(model[\"transformer\"])\n",
    "\n",
    "    if not restart_training:\n",
    "        optimizer.load_state_dict(model[\"optim\"])\n",
    "        start_epoch = model.get(\"epoch\", 0)\n",
    "    logger.info(f\"Model {model_name} loaded with {count_parameters(transformer)} Parameters\")\n",
    "else: \n",
    "    logger.info(f\"Model {model_name} created with {count_parameters(transformer)} Parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecfb1f7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "758f344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataset(Dataset):\n",
    "    def __init__(self, num_sequences=1000, seq_len=32768, vocab_size=2048, period=10):\n",
    "        self.num_sequences = num_sequences\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.period = period\n",
    "        self.data = self._generate_data()\n",
    "\n",
    "    def _generate_data(self):\n",
    "        # Generate periodic sequences: [0, 1, 2, ..., period-1, 0, 1, ...]\n",
    "        pattern = torch.arange(self.period) % self.vocab_size\n",
    "        sequences = pattern.repeat(self.seq_len // self.period + 1)[:self.seq_len]\n",
    "        data = sequences.repeat(self.num_sequences, 1)  # [num_sequences, seq_len]\n",
    "        # Add slight noise to mimic variability\n",
    "        noise_mask = torch.rand(self.num_sequences, self.seq_len) < 0.05  # 5% noise\n",
    "        random_tokens = torch.randint(0, self.vocab_size, (self.num_sequences, self.seq_len))\n",
    "        data = torch.where(noise_mask, random_tokens, data)\n",
    "        return data.to(torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_sequences\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], 0\n",
    "\n",
    "#train_dataset = SyntheticDataset(num_sequences=800, seq_len=32768, vocab_size=2048, period=10)\n",
    "#train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24a8a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fad398e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Training started on {device}\")\n",
    "loss_list: list = []\n",
    "total_time: float = 0.0\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "transformer.train()\n",
    "for e in range(n_epochs):\n",
    "    total_loss: float = 0\n",
    "    total_accuracy: float = 0\n",
    "    start_time: float = time.time()\n",
    "\n",
    "    for b_idx, (x, _) in enumerate(train_dataloader):\n",
    "        if x.size(1) > training_seq_len:\n",
    "            start_idx = torch.randint(0, x.size(1) - training_seq_len + 1, (1,)).item()\n",
    "            x = x[:, start_idx:start_idx + training_seq_len]\n",
    "        x = x.to(device)\n",
    "        inp = x[:, :-1]\n",
    "        target = x[:, 1:]\n",
    "        with torch.autocast(device):\n",
    "            pred = transformer(inp)\n",
    "            loss = loss_fn(pred.transpose(1, 2), target)\n",
    "            pred_indices = pred.argmax(dim=-1)\n",
    "            correct = (pred_indices == target)\n",
    "            correct_count = correct.sum().item()\n",
    "            total_count = inp.shape[0] * inp.shape[1]\n",
    "            accuracy = correct_count / total_count\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss\n",
    "        total_accuracy += accuracy\n",
    "        total_norm = sum(p.grad.norm(2).item() ** 2 for p in transformer.parameters() if p.grad is not None) ** 0.5\n",
    "        #torch.nn.utils.clip_grad_norm_(transformer.parameters(), max_norm=1.0)\n",
    "        \n",
    "        if b_idx == 0:\n",
    "            probs = F.softmax(pred[:, -1, :], dim=-1)\n",
    "            #logger.info(f\"Epoch {e + 1}, Batch {b_idx + 1}: Pred min: {pred.min().item():.4f}, max: {pred.max().item():.4f}, Probs max: {probs.max().item():.4f} Tot. norm unclipped: {total_norm:.3e}\")\n",
    "        scheduler.step()\n",
    "\n",
    "    transformer.eval()\n",
    "    total_val_loss: float = 0\n",
    "    total_val_accuracy: float = 0\n",
    "    for x,_ in validation_dataloader:\n",
    "        if x.size(1) > training_seq_len:\n",
    "            start_idx = torch.randint(0, x.size(1) - training_seq_len + 1, (1,)).item()\n",
    "            x = x[:, start_idx:start_idx + training_seq_len]\n",
    "        x = x.to(device)\n",
    "        inp = x[:, :-1]\n",
    "        target = x[:, 1:]\n",
    "        with torch.no_grad():\n",
    "            pred = transformer(inp)\n",
    "            total_val_loss += loss_fn(pred.transpose(1, 2), target)\n",
    "            pred_indices = pred.argmax(dim=-1)\n",
    "            correct = (pred_indices == target)\n",
    "            correct_count = correct.sum().item()\n",
    "            total_count = inp.shape[0] * inp.shape[1]\n",
    "            total_val_accuracy += correct_count / total_count\n",
    "    transformer.train()\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    total_time += epoch_time\n",
    "    remaining_time = int((total_time / (e + 1)) * (n_epochs - e - 1))\n",
    "    avg_accuracy = total_accuracy / len(train_dataloader) * 100\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    avg_val_accuracy = total_val_accuracy / len(validation_dataloader) * 100\n",
    "    avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "    loss_list.append({\"loss\": avg_loss, \"accuracy\": avg_accuracy, \"val_loss\": avg_val_loss, \"val_accuracy\": avg_val_accuracy})\n",
    "    logger.info(f\"Epoch {e + 1:03d}: Avg. Loss: {avg_loss:.3e} Avg. Accuracy: {avg_accuracy:.3f}% Avg. val Loss: {avg_val_loss:.3e} Avg. val Accuracy: {avg_val_accuracy:.3f}% Remaining Time: {remaining_time // 3600:02d}h {(remaining_time % 3600) // 60:02d}min {round(remaining_time % 60):02d}s LR: {optimizer.param_groups[0]['lr']:.3e}\")\n",
    "\n",
    "    if checkpoint_freq > 0 and (e + 1) % checkpoint_freq == 0:\n",
    "        checkpoint_path: str = f\"{full_model_path[:-4]}_epoch_{e + 1:03d}.pth\"\n",
    "        torch.save({\"transformer\": transformer.state_dict(), \"optim\": optimizer.state_dict(), \"epoch\": e + 1}, checkpoint_path)\n",
    "        if e + 1 != checkpoint_freq:\n",
    "            last_path: str = f\"{full_model_path[:-4]}_epoch_{(e + 1) - checkpoint_freq:03d}.pth\"\n",
    "            OS().del_if_exists(last_path)\n",
    "        logger.light_debug(f\"Checkpoint saved model to {checkpoint_path}\")\n",
    "\n",
    "torch.save({\"transformer\": transformer.state_dict(), \"optim\": optimizer.state_dict(), \"epoch\": e + 1}, full_model_path)\n",
    "\n",
    "if checkpoint_freq > 0:\n",
    "    checkpoint_path: str = f\"{full_model_path[:-4]}_epoch_{e + 1 - ((e + 1) % checkpoint_freq):03d}.pth\"\n",
    "    OS().del_if_exists(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "752077c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = [entry[\"loss\"].detach().cpu().numpy() for entry in loss_list]\n",
    "val_loss = [entry[\"val_loss\"].cpu().numpy() for entry in loss_list]\n",
    "train_acc = [entry[\"accuracy\"] for entry in loss_list]\n",
    "val_acc = [entry[\"val_accuracy\"] for entry in loss_list]\n",
    "\n",
    "# Create figure\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot losses (left y-axis)\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\", color=\"tab:red\")\n",
    "ax1.plot(range(n_epochs), train_loss, label=\"Train Loss\", color=\"tab:red\", linestyle=\"-\")\n",
    "ax1.plot(range(n_epochs), val_loss, label=\"Val Loss\", color=\"tab:orange\", linestyle=\"--\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "\n",
    "# Second y-axis for accuracy\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Accuracy\", color=\"tab:blue\")\n",
    "ax2.plot(range(n_epochs), train_acc, label=\"Train Accuracy\", color=\"tab:blue\", linestyle=\"-\")\n",
    "ax2.plot(range(n_epochs), val_acc, label=\"Val Accuracy\", color=\"tab:cyan\", linestyle=\"--\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "# Combine legends from both axes\n",
    "lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc=\"center right\")\n",
    "\n",
    "plt.title(\"Training & Validation Loss and Accuracy\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feffa1a",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef1b6a",
   "metadata": {},
   "source": [
    "### Init & Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aded995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = GPTDecoder().to(device)\n",
    "vq_vae = VQVAE(input_emb_width=1, output_emb_width=64, k_bins=2048, levels=1, downs_t=[3], strides_t=[2]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1218a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(full_model_path):\n",
    "    model = torch.load(full_model_path, map_location=device)\n",
    "    transformer.load_state_dict(model[\"transformer\"])\n",
    "    logger.info(f\"Model {model_name} loaded with {count_parameters(transformer)} Parameters\")\n",
    "if os.path.exists(vq_vae_path):\n",
    "    model = torch.load(vq_vae_path, map_location=device)\n",
    "    vq_vae.load_state_dict(model[\"vq_vae\"])\n",
    "    logger.info(f\"Model {vq_vae_name} loaded with {count_parameters(vq_vae)} Parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023b6382",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1213efdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(vq_vae: VQVAE, transformer: GPTDecoder, context: Tensor, device: str = \"cpu\", temperature: float = 1.0, top_k: (int | None) = 50, total_samples: int = 2 ** 15) -> ndarray:\n",
    "    vq_vae.eval()\n",
    "    transformer.eval()\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        if context.ndim == 1:\n",
    "            context = context.unsqueeze(0)\n",
    "        if context.ndim == 3:\n",
    "            context = context.squeeze(1)\n",
    "        sequence = context.to(device)\n",
    "        while sequence.size(-1) < total_samples:\n",
    "            logits = transformer(sequence[..., -transformer.max_seq_len:])\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float('inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            sequence = torch.cat([sequence, next_token], dim=-1)\n",
    "            dt = time.time() - start_time\n",
    "            print(f\"\\rGenerated {sequence.size(-1)}/{total_samples} Samples {int((sequence.size(-1) - context.size(-1)) / dt)} tk/s\", end='', flush=True)\n",
    "        print(flush=True)\n",
    "        sequence = sequence[..., :total_samples]\n",
    "        if sequence.ndim == 3:\n",
    "            sequence = sequence.squeeze(1)\n",
    "        print(sequence)\n",
    "        print(context)\n",
    "        audio = vq_vae.decode(sequence)\n",
    "        return audio.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c569be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader_item(dataloader: DataLoader, idx: int = 0) -> Tensor:\n",
    "    n = 0\n",
    "    for x,y in dataloader:\n",
    "        b_size = x.size(0)\n",
    "        \n",
    "        if n + b_size > idx:\n",
    "            return x[idx - n], y[idx - n]\n",
    "        n += b_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74669abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = latent_test_dataset[:24][0].to(device)\n",
    "context = context.unsqueeze(1)\n",
    "print(context.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d014da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context[0])\n",
    "audio = sample(vq_vae, transformer, context[:10,..., :32700], device, temperature=1, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "045da239",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "n = 1\n",
    "vq_vae.eval()\n",
    "inp = test_dataset[:24][0].unsqueeze(1).to(device)\n",
    "print(inp.shape)\n",
    "with torch.no_grad():\n",
    "    output, q_z, commit_loss, metrics = vq_vae(inp)\n",
    "    outp = vq_vae.decode(q_z)\n",
    "audio = sample(vq_vae, transformer, q_z[:12,...,:30000], device, temperature=1, top_k=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "289780f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_dataloader_item(latent_test_dataloader, 5)[0]\n",
    "data = data.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47021601",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = sample(vq_vae, transformer, data[..., :30000], device, temperature=1, top_k=50)\n",
    "vq_vae.eval()\n",
    "with torch.no_grad():\n",
    "    test = vq_vae.decode([data.to(device)])\n",
    "plt.plot(audio[0][0])\n",
    "plt.title(\"Transformer Output\")\n",
    "plt.show()\n",
    "plt.plot(test[0][0].cpu().numpy())\n",
    "plt.title(\"VQ-VAE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4848a88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(inp[idx][0].cpu().numpy())\n",
    "plt.title(\"Input\")\n",
    "plt.show()\n",
    "plt.plot(output[idx][0].cpu().numpy())\n",
    "plt.title(\"Seperatly decoded Inp\")\n",
    "plt.show()\n",
    "plt.plot(audio[5][0])\n",
    "plt.title(\"Transformer Output\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5145a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "plt.plot(audio[idx][0])\n",
    "plt.show()\n",
    "plt.plot(get_dataloader_item(test_dataloader, idx)[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e30a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AudioData(data = audio[4][0]).save_audio_file(\"Results/Jukebox_test5.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
